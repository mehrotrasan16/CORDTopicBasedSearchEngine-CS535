{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a need to respond quickly to changes in today's world. This response may sometimes require understanding of a topic. Given any topic, there is a plethora of sources providing seas of data related to the it. Given todays world condition, a global concern is the treatment and handling of the COVID-19 pandemic. Given the rising amount of literature regarding the subject, it will be difficult for a scientist to keep up with today's literature.\n",
    "> Is it possible to cluster these tens of thousands of articles to make it easier for a scientist to find relevant research articles?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for imports only: 0:00:01.880153\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "start_time = datetime.now()\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from hdfs import InsecureClient\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(f'Time for imports only: {end_time-start_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading all files into Spark RDDs with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setAppName(\"SparkTFIDF\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "hdfs_folder = 'hdfs://madison:31802/cord19dataset/metadata.csv'\n",
    "\n",
    "def read_data(hdfs_folder):\n",
    "    data = sc.parallelize([])\n",
    "    data = sc.textFile(hdfs_folder)\n",
    "    return data\n",
    "\n",
    "meta = read_data(hdfs_folder)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hdfs://madison:31802/cord19dataset/metadata.csv MapPartitionsRDD[2] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading all files from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_hdfs = InsecureClient('http://madison:31802')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'directoryCount': 17,\n",
       " 'fileCount': 59317,\n",
       " 'length': 7913288034,\n",
       " 'quota': -1,\n",
       " 'spaceConsumed': 23739864102,\n",
       " 'spaceQuota': -1,\n",
       " 'typeQuota': {}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_hdfs.content('/cord19dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['COVID.DATA.LIC.AGMT.pdf',\n",
       " 'biorxiv_medrxiv',\n",
       " 'biorxiv_medrxiv.tar.gz',\n",
       " 'comm_use_subset',\n",
       " 'cord19_specter_embeddings_2020-04-10',\n",
       " 'custom_license',\n",
       " 'json_schema.txt',\n",
       " 'metadata.csv',\n",
       " 'metadata.readme',\n",
       " 'noncomm_use_subset']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames = client_hdfs.list('/cord19dataset')\n",
    "fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0015023cc06b5362d332b3baf348d11567ca2fbb.json',\n",
       " '00340eea543336d54adda18236424de6a5e91c9d.json',\n",
       " '004f0f8bb66cf446678dc13cf2701feec4f36d76.json',\n",
       " '00911cf4f99a3d5ae5e5b787675646a743574496.json',\n",
       " '00d16927588fb04d4be0e6b269fc02f0d3c2aa7b.json',\n",
       " '00eb9220dc8cd351393b6b035323d350f103f8c6.json',\n",
       " '0139ea4ca580af99b602c6435368e7fdbefacb03.json',\n",
       " '013d9d1cba8a54d5d3718c229b812d7cf91b6c89.json',\n",
       " '018fb5e62fbbcae07d57d94d29ac630dcc4dccf9.json',\n",
       " '01d162d7fae6aaba8e6e60e563ef4c2fca7b0e18.json',\n",
       " '01e3b313e78a352593be2ff64927192af66619b5.json',\n",
       " '02201e4601ab0eb70b6c26480cf2bfeae2625193.json',\n",
       " '0255ea4b2f26a51a3bfa3bd8f3e1978c82c976d5.json',\n",
       " '029c1c588047f1d612a219ee15494d2d19ff7439.json',\n",
       " '0313c3faa16cd66d64f31ae37e40fb70695d69fb.json',\n",
       " '033ea7af3e6137df652de026f0751ac435327b75.json',\n",
       " '03813d8657ba43ea382788caec2d14257b26d8fd.json',\n",
       " '03ce432f27c7df6af22b92245a614db2ecb5de5f.json',\n",
       " '03ea3a614b56409d3f099c9ad764864293132540.json',\n",
       " '03ea9ad47ebe9a599205b99390c45490e6724024.json',\n",
       " '04030bba3035a58c7725ae267973206f6eb6c0b4.json',\n",
       " '041bae0a6de2b69979d39460b3f2ee8946534ec2.json',\n",
       " '05082393ba4c7ec530190dd887d99c74fd72f6d6.json',\n",
       " '052bf4fb7deaf593862991af3b118b5f11a9fbe1.json',\n",
       " '0537863d6c059cc9cf5ec8dd0beb27dec5d9d801.json',\n",
       " '05d47dd5b46f86428de058db4ecc8bca76a9ad16.json',\n",
       " '05d99c07db59b6948e39bfa62c2cbbf62944059a.json',\n",
       " '05e37847597676ae715adcec18a6574e75a20546.json',\n",
       " '061ffcdd4d674c4d7ce24e4aa7c5037c68596864.json',\n",
       " '0624a12abfe85c8b5070850d912a2db4cd453236.json',\n",
       " '06837008df793f872a6fb830dfb83c9525edb7c4.json',\n",
       " '06a1002f9fbea7179ac3572843f66b14568af6e4.json',\n",
       " '06acb8da0009104a2af509334abe3c26b1da66a1.json',\n",
       " '06c1b3535b83251cf92c01258b5048beeab7a460.json',\n",
       " '06d12dc5ac32d82387c65370d0a600e13059122d.json',\n",
       " '073d74442e2655d79b0b3f764a627ec667ad422c.json',\n",
       " '07e833d0917cace550853f72923856d0fe1a7120.json',\n",
       " '08660499ee722a74043f8417faee3e1eeb9d0f5f.json',\n",
       " '08826d0596a01a2a482eb6d80edb0e87cddc304e.json',\n",
       " '08911cdc65e71e6398ca79b46806e6c8b2b730ae.json',\n",
       " '08a22278486e12768ce186677a6a89663d24586f.json',\n",
       " '090b6c8b3df30bc248221869f673a2d970caa1b9.json',\n",
       " '091a8e9a61e19e88caeb039f0e3888d111b20439.json',\n",
       " '09892e597bdc1ded9eb5922b4f3d41e041d6634a.json',\n",
       " '09b6706748f0c1ae0da436ac2dfac9052b84e4ea.json',\n",
       " '09c9fcabc66a106e01ef42247cbd86b6d85bd67f.json',\n",
       " '09ec8daa8e32168d92d05b86de1784c639685fb4.json',\n",
       " '09fd48d0e1f60fa69b68ebd54bd5d71fc08dec96.json',\n",
       " '0a27cb2cd52229472fcfc3e49d3a3cb7179867e4.json',\n",
       " '0a2a28cb82e7a03af0a9fad4fd4c68c9fdac2477.json']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames = client_hdfs.list('/cord19dataset/biorxiv_medrxiv/biorxiv_medrxiv/pdf_json')\n",
    "fnames[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##It is possible to loop through all the folders and sub folders while using \n",
    "##content(<path>,strict=False) != None as a breaking condition`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59317"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#There is also this provided snippet to get all files under a given directory\n",
    "# Get all files under a given folder (arbitrary depth).\n",
    "import posixpath as psp\n",
    "fpaths = [\n",
    "  psp.join(dpath, fname)\n",
    "  for dpath, _, fnames in client_hdfs.walk('/cord19dataset')\n",
    "  for fname in fnames\n",
    "]\n",
    "len(fpaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59311"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpaths_json = list(filter(lambda x: \".json\" in x,fpaths))\n",
    "\n",
    "len(fpaths_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "_125hdfsjson = fpaths_json[0:12500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata for the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are given a metadata file that gives us the filename of each article along with its abstract and some other details such as date published and journal that it was published in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '/cord19dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>sha</th>\n",
       "      <th>source_x</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>pmcid</th>\n",
       "      <th>pubmed_id</th>\n",
       "      <th>license</th>\n",
       "      <th>abstract</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal</th>\n",
       "      <th>Microsoft Academic Paper ID</th>\n",
       "      <th>WHO #Covidence</th>\n",
       "      <th>has_pdf_parse</th>\n",
       "      <th>has_pmc_xml_parse</th>\n",
       "      <th>full_text_file</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xqhn0vbp</td>\n",
       "      <td>1e1286db212100993d03cc22374b624f7caee956</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Airborne rhinovirus detection and effect of ul...</td>\n",
       "      <td>10.1186/1471-2458-3-5</td>\n",
       "      <td>PMC140314</td>\n",
       "      <td>12525263.0</td>\n",
       "      <td>no-cc</td>\n",
       "      <td>BACKGROUND: Rhinovirus, the most common cause ...</td>\n",
       "      <td>2003-01-13</td>\n",
       "      <td>Myatt, Theodore A; Johnston, Sebastian L; Rudn...</td>\n",
       "      <td>BMC Public Health</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gi6uaa83</td>\n",
       "      <td>8ae137c8da1607b3a8e4c946c07ca8bda67f88ac</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Discovering human history from stomach bacteria</td>\n",
       "      <td>10.1186/gb-2003-4-5-213</td>\n",
       "      <td>PMC156578</td>\n",
       "      <td>12734001.0</td>\n",
       "      <td>no-cc</td>\n",
       "      <td>Recent analyses of human pathogens have reveal...</td>\n",
       "      <td>2003-04-28</td>\n",
       "      <td>Disotell, Todd R</td>\n",
       "      <td>Genome Biol</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>le0ogx1s</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PMC</td>\n",
       "      <td>A new recruit for the army of the men of death</td>\n",
       "      <td>10.1186/gb-2003-4-7-113</td>\n",
       "      <td>PMC193621</td>\n",
       "      <td>12844350.0</td>\n",
       "      <td>no-cc</td>\n",
       "      <td>The army of the men of death, in John Bunyan's...</td>\n",
       "      <td>2003-06-27</td>\n",
       "      <td>Petsko, Gregory A</td>\n",
       "      <td>Genome Biol</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fy4w7xz8</td>\n",
       "      <td>0104f6ceccf92ae8567a0102f89cbb976969a774</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Association of HLA class I with severe acute r...</td>\n",
       "      <td>10.1186/1471-2350-4-9</td>\n",
       "      <td>PMC212558</td>\n",
       "      <td>12969506.0</td>\n",
       "      <td>no-cc</td>\n",
       "      <td>BACKGROUND: The human leukocyte antigen (HLA) ...</td>\n",
       "      <td>2003-09-12</td>\n",
       "      <td>Lin, Marie; Tseng, Hsiang-Kuang; Trejaut, Jean...</td>\n",
       "      <td>BMC Med Genet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0qaoam29</td>\n",
       "      <td>5b68a553a7cbbea13472721cd1ad617d42b40c26</td>\n",
       "      <td>PMC</td>\n",
       "      <td>A double epidemic model for the SARS propagation</td>\n",
       "      <td>10.1186/1471-2334-3-19</td>\n",
       "      <td>PMC222908</td>\n",
       "      <td>12964944.0</td>\n",
       "      <td>no-cc</td>\n",
       "      <td>BACKGROUND: An epidemic of a Severe Acute Resp...</td>\n",
       "      <td>2003-09-10</td>\n",
       "      <td>Ng, Tuen Wai; Turinici, Gabriel; Danchin, Antoine</td>\n",
       "      <td>BMC Infect Dis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cord_uid                                       sha source_x  \\\n",
       "0  xqhn0vbp  1e1286db212100993d03cc22374b624f7caee956      PMC   \n",
       "1  gi6uaa83  8ae137c8da1607b3a8e4c946c07ca8bda67f88ac      PMC   \n",
       "2  le0ogx1s                                       NaN      PMC   \n",
       "3  fy4w7xz8  0104f6ceccf92ae8567a0102f89cbb976969a774      PMC   \n",
       "4  0qaoam29  5b68a553a7cbbea13472721cd1ad617d42b40c26      PMC   \n",
       "\n",
       "                                               title                      doi  \\\n",
       "0  Airborne rhinovirus detection and effect of ul...    10.1186/1471-2458-3-5   \n",
       "1    Discovering human history from stomach bacteria  10.1186/gb-2003-4-5-213   \n",
       "2     A new recruit for the army of the men of death  10.1186/gb-2003-4-7-113   \n",
       "3  Association of HLA class I with severe acute r...    10.1186/1471-2350-4-9   \n",
       "4   A double epidemic model for the SARS propagation   10.1186/1471-2334-3-19   \n",
       "\n",
       "       pmcid   pubmed_id license  \\\n",
       "0  PMC140314  12525263.0   no-cc   \n",
       "1  PMC156578  12734001.0   no-cc   \n",
       "2  PMC193621  12844350.0   no-cc   \n",
       "3  PMC212558  12969506.0   no-cc   \n",
       "4  PMC222908  12964944.0   no-cc   \n",
       "\n",
       "                                            abstract publish_time  \\\n",
       "0  BACKGROUND: Rhinovirus, the most common cause ...   2003-01-13   \n",
       "1  Recent analyses of human pathogens have reveal...   2003-04-28   \n",
       "2  The army of the men of death, in John Bunyan's...   2003-06-27   \n",
       "3  BACKGROUND: The human leukocyte antigen (HLA) ...   2003-09-12   \n",
       "4  BACKGROUND: An epidemic of a Severe Acute Resp...   2003-09-10   \n",
       "\n",
       "                                             authors            journal  \\\n",
       "0  Myatt, Theodore A; Johnston, Sebastian L; Rudn...  BMC Public Health   \n",
       "1                                   Disotell, Todd R        Genome Biol   \n",
       "2                                  Petsko, Gregory A        Genome Biol   \n",
       "3  Lin, Marie; Tseng, Hsiang-Kuang; Trejaut, Jean...      BMC Med Genet   \n",
       "4  Ng, Tuen Wai; Turinici, Gabriel; Danchin, Antoine     BMC Infect Dis   \n",
       "\n",
       "   Microsoft Academic Paper ID WHO #Covidence  has_pdf_parse  \\\n",
       "0                          NaN            NaN           True   \n",
       "1                          NaN            NaN           True   \n",
       "2                          NaN            NaN          False   \n",
       "3                          NaN            NaN           True   \n",
       "4                          NaN            NaN           True   \n",
       "\n",
       "   has_pmc_xml_parse  full_text_file  \\\n",
       "0               True  custom_license   \n",
       "1               True  custom_license   \n",
       "2               True  custom_license   \n",
       "3               True  custom_license   \n",
       "4               True  custom_license   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1...  \n",
       "1  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1...  \n",
       "2  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1...  \n",
       "3  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2...  \n",
       "4  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with client_hdfs.read(f'{root_path}/metadata.csv', encoding = 'utf-8') as reader:\n",
    "    meta = pd.read_csv(reader)\n",
    "\n",
    "meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cord_uid                       51078\n",
       "sha                            38022\n",
       "source_x                       51078\n",
       "title                          50920\n",
       "doi                            47741\n",
       "pmcid                          41082\n",
       "pubmed_id                      37861\n",
       "license                        51078\n",
       "abstract                       42352\n",
       "publish_time                   51070\n",
       "authors                        48891\n",
       "journal                        46368\n",
       "Microsoft Academic Paper ID      964\n",
       "WHO #Covidence                  1768\n",
       "has_pdf_parse                  51078\n",
       "has_pmc_xml_parse              51078\n",
       "full_text_file                 42511\n",
       "url                            50776\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51078 entries, 0 to 51077\n",
      "Data columns (total 18 columns):\n",
      "cord_uid                       51078 non-null object\n",
      "sha                            38022 non-null object\n",
      "source_x                       51078 non-null object\n",
      "title                          50920 non-null object\n",
      "doi                            47741 non-null object\n",
      "pmcid                          41082 non-null object\n",
      "pubmed_id                      37861 non-null float64\n",
      "license                        51078 non-null object\n",
      "abstract                       42352 non-null object\n",
      "publish_time                   51070 non-null object\n",
      "authors                        48891 non-null object\n",
      "journal                        46368 non-null object\n",
      "Microsoft Academic Paper ID    964 non-null float64\n",
      "WHO #Covidence                 1768 non-null object\n",
      "has_pdf_parse                  51078 non-null bool\n",
      "has_pmc_xml_parse              51078 non-null bool\n",
      "full_text_file                 42511 non-null object\n",
      "url                            50776 non-null object\n",
      "dtypes: bool(2), float64(2), object(14)\n",
      "memory usage: 6.3+ MB\n"
     ]
    }
   ],
   "source": [
    "meta.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the column marked \"SHA\". At a closer look, we can see that it seems to be a hash of some kind.\n",
    "\n",
    "But looking into the folders, we see that this unique hash has been ingeniously used to identify each file. i.e. We don't need to refer to a long paper title to find its file contents. Instead we can just scan the home directory for a file with that hash.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1e1286db212100993d03cc22374b624f7caee956'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta[\"sha\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'daf32e013d325a6feb80e83d15aabc64a48fae33'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bio= meta.loc[meta['source_x'] == 'biorxiv']\n",
    "bio.head()\n",
    "bio.iloc[1]['sha']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the source_x column is of importance for us as well. It refers to the source journal of each paper. This is important because the papers have been organized into folders, each representing a different source. e.g. a biorxiv folder, a WHO document folder etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['PMC', 'Elsevier', 'CZI', 'WHO', 'biorxiv', 'medrxiv'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.source_x.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us read some of the biorxiv files into variables to see the structure as viewed in the screenshot before section 2.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'paper_id': '0015023cc06b5362d332b3baf348d11567ca2fbb', 'metadata': {'title': 'The RNA pseudoknots in foot-and-mouth disease virus are dispensable for genome replication but essential for the production of infectious virus. 2 3', 'authors': [{'first': 'Joseph', 'middle': ['C'], 'last': 'Ward', 'suffix': '', 'affiliation': {}, 'email': ''}, {'first': 'Lidia', 'middle': [], 'last': 'Lasecka-Dykes', 'suffix': '', 'affiliation': {}, 'email': ''}, {'first': 'Chris', 'middle': [], 'last': 'Neil', 'suffix': '', 'affiliation': {}, 'email': ''}, {'first': 'Oluwapelumi', 'middle': [], 'last': 'Adeyemi', 'suffix': '', 'affiliation': {}, 'email': ''}, {'first': 'Sarah', 'middle': [], 'last': '', 'suffix': '', 'affiliation': {}, 'email': ''}, {'first': '', 'middle': [], 'last': 'Gold', 'suffix': '', 'affiliation': {}, 'email': ''}, {'first': 'Niall', 'middle': [], 'last': 'Mclean', 'suffix': '', 'affiliation': {}, 'email': ''}, {'first': 'Caroline', 'middle': [], 'last': 'Wright', 'suffix': '', 'affiliation': {}, 'email': ''}, {'first': 'Morgan', 'middle': ['R'], 'last': 'Herod', 'suffix': '', 'affiliation': {}, 'email': ''}, {'first': 'David', 'middle': [], 'last': 'Kealy', 'suffix': '', 'affiliation': {}, 'email': ''}, {'first': 'Emma', 'middle': [], 'last': '', 'suffix': '', 'affiliation': {}, 'email': ''}, {'first': 'Warner', 'middle': [], 'last': '', 'suffix': '', 'affiliation': {}, 'email': ''}, {'first': 'Donald', 'middle': ['P'], 'last': 'King', 'suffix': '', 'affiliation': {}, 'email': ''}, {'first': 'Tobias', 'middle': ['J'], 'last': 'Tuthill', 'suffix': '', 'affiliation': {}, 'email': ''}, {'first': 'David', 'middle': ['J'], 'last': 'Rowlands', 'suffix': '', 'affiliation': {}, 'email': ''}, {'first': 'Nicola', 'middle': ['J'], 'last': '', 'suffix': '', 'affiliation': {}, 'email': ''}, {'first': 'Stonehouse', 'middle': [], 'last': 'A#', 'suffix': '', 'affiliation': {}, 'email': ''}]}, 'abstract': [{'text': 'word count: 194 22 Text word count: 5168 23 24 25 author/funder. All rights reserved. No reuse allowed without permission. Abstract 27 The positive stranded RNA genomes of picornaviruses comprise a single large open reading 28 frame flanked by 5′ and 3′ untranslated regions (UTRs). Foot-and-mouth disease virus (FMDV) 29 has an unusually large 5′ UTR (1.3 kb) containing five structural domains. These include the 30 internal ribosome entry site (IRES), which facilitates initiation of translation, and the cis-acting 31 replication element (cre). Less well characterised structures are a 5′ terminal 360 nucleotide 32 stem-loop, a variable length poly-C-tract of approximately 100-200 nucleotides and a series of 33 two to four tandemly repeated pseudoknots (PKs). We investigated the structures of the PKs 34 by selective 2′ hydroxyl acetylation analysed by primer extension (SHAPE) analysis and 35 determined their contribution to genome replication by mutation and deletion experiments. 36 SHAPE and mutation experiments confirmed the importance of the previously predicted PK 37 structures for their function. Deletion experiments showed that although PKs are not essential 38', 'cite_spans': [], 'ref_spans': [], 'section': 'Abstract'}, {'text': 'for replication, they provide genomes with a competitive advantage. However, although 39 replicons and full-length genomes lacking all PKs were replication competent, no infectious 40 virus was rescued from genomes containing less than one PK copy. This is consistent with our 41 earlier report describing the presence of putative packaging signals in the PK region. 42 43 author/funder. All rights reserved. No reuse allowed without permission.', 'cite_spans': [], 'ref_spans': [], 'section': 'Abstract'}], 'body_text': [{'text': 'VP3, and VP0 (which is further processed to VP2 and VP4 during virus assembly) (6). The P2 64 and P3 regions encode the non-structural proteins 2B and 2C and 3A, 3B (1-3) (VPg), 3C pro and 4 structural protein-coding region is replaced by reporter genes, allow the study of genome 68 replication without the requirement for high containment (9, 10) ( figure 1A ).', 'cite_spans': [], 'ref_spans': [{'start': 351, 'end': 360, 'text': 'figure 1A', 'ref_id': 'FIGREF50'}], 'section': ''}, {'text': 'The FMDV 5′ UTR is the largest known picornavirus UTR, comprising approximately 1300 71 nucleotides and containing several highly structured regions. The first 360 nucleotides at the 5′ 72 end are predicted to fold into a single large stem loop termed the S-fragment, followed by a The PKs were originally predicted in 1987 and consist of two to four tandem repeats of a ~48 86 nucleotide region containing a small stem loop and downstream interaction site (figure 1B) 87 (12). Due to the sequence similarity between the PKs (figure 1C), it is speculated that they 88 were formed by duplication events during viral replication, probably involving recombination. 89 Between two and four PKs are present in different virus isolates but no strain has been 90 identified with less than two PKs, emphasising their potential importance in the viral life cycle 91 (19, 20) . The presence of PKs has been reported in the 5′ UTR of other picornaviruses such as 92 author/funder. All rights reserved. No reuse allowed without permission. can occur in the absence of PKs at least one is required for wild-type (wt) replication. 104 Furthermore, competition experiments showed that extra copies of PKs conferred a replicative 105 advantage to genomes. Although replicons and full-length genomes lacking PKs were 106 replication-competent, no infectious virus was rescued from genomes containing less than one 107 PK copy. This is consistent with our earlier report describing the presence of putative 108 packaging signals in the PK region (22). 109 110 author/funder. All rights reserved. No reuse allowed without permission. Plasmid construction. 117 The FMDV replicon plasmids, pRep-ptGFP, and the replication-defective polymerase mutant 118 control, 3D-GNN, have already been described (10).', 'cite_spans': [{'start': 469, 'end': 471, 'text': '87', 'ref_id': None}, {'start': 662, 'end': 664, 'text': '89', 'ref_id': None}, {'start': 857, 'end': 861, 'text': '(19,', 'ref_id': None}, {'start': 862, 'end': 865, 'text': '20)', 'ref_id': None}, {'start': 1117, 'end': 1120, 'text': '104', 'ref_id': None}, {'start': 1637, 'end': 1640, 'text': '117', 'ref_id': None}], 'ref_spans': [], 'section': '70'}, {'text': 'To introduce mutations into the PK region, the pRep-ptGFP replicon plasmid was digested 121 with SpeI and KpnI and the resulting fragment inserted into a sub-cloning vector (pBluescript) 122 to create the pBluescript PK. PKs 3 and 4 were removed by digestion with HindIII and AatII 123 before insertion of a synthetic DNA sequence with PK 3 and 4 deleted. PKs 2, 3 and 4 were 124 deleted by PCR amplification using ΔPK 234 Forward primer and FMDV 1331-1311 reverse 125 primer, the resultant product was digested with HindIII and AatII and ligated into the 126 pBluescript PK vector. Complete PK deletion was achieved by introduction of an AflII site at 127 the 3′ end of the poly-C tract by PCR mutagenesis to create the sub-cloning vector, pBluescript 128 C11, which was then used to remove all the PKs by PCR mutagenesis using ΔPK 1234 forward 129 primer and FMDV 1331-1311 reverse primer. The modified PK sequences were removed from 130 the sub-cloning vectors and inserted into the pRep-ptGFP plasmid using NheI-HF and KpnI-131 HF.', 'cite_spans': [], 'ref_spans': [], 'section': '120'}, {'text': '132 133 author/funder. All rights reserved. No reuse allowed without permission.', 'cite_spans': [], 'ref_spans': [], 'section': '120'}, {'text': 'The copyright holder for this preprint (which was not peer-reviewed) is the . https://doi.org/10.1101/2020.01.10.901801 doi: bioRxiv preprint 7 Mutations to disrupt and reform PK structure were introduced using synthetic DNA by 134 digestion with AflII and AatII and ligation into a similarly digested pBluescript PK vector.', 'cite_spans': [], 'ref_spans': [], 'section': '120'}, {'text': 'Mutations were then introduced into the replicon plasmid as described above.', 'cite_spans': [], 'ref_spans': [], 'section': '135'}, {'text': 'To assess the effects of truncation of the poly-C-tract on replication the entire sequence was 137 removed. This was performed by PCR mutagenesis using primers C0 SpeI, and FMDV 1331- In vitro transcription. 143 In vitro transcription reactions for replicon assays were performed as described previously (28).', 'cite_spans': [{'start': 208, 'end': 211, 'text': '143', 'ref_id': None}], 'ref_spans': [], 'section': '136'}, {'text': 'Transcription reactions to produce large amounts of RNA for SHAPE analysis were performed 145 with purified linear DNA as described above, and 1 μg of linearised DNA was then used in a 146 HiScribe T7 synthesis kit (NEB), before DNase treatment and purification using a PureLink FastQ files were quality checked using FastQC with poor quality reads filtered using the 225 Sickle algorithm. Host cell reads were removed using FastQ Screen algorithm and FMDV 226 reads assembled de novo into contigs using IDBA-UD (35). Contigs that matched the FMDV 227 library (identified using Basic Local ALighnment Search Tool (BLAST)) were assembled 228 author/funder. All rights reserved. No reuse allowed without permission.', 'cite_spans': [{'start': 368, 'end': 371, 'text': '225', 'ref_id': None}], 'ref_spans': [], 'section': '144'}, {'text': 'The copyright holder for this preprint (which was not peer-reviewed) is the . https://doi.org/10.1101/2020.01.10.901801 doi: bioRxiv preprint into consensus sequences using SeqMan Pro software in the DNA STAR Lasergene 13 229 package (DNA STAR) (36). The SHAPE data largely agreed with the predicted structures with the stems of PK 1, 2 and 3, interacting nucleotides showed little to no reactivity, suggesting NMIA could not interact with 300 author/funder. All rights reserved. No reuse allowed without permission.', 'cite_spans': [], 'ref_spans': [], 'section': '144'}, {'text': 'The copyright holder for this preprint (which was not peer-reviewed) is the . https://doi.org/10.1101/2020.01.10.901801 doi: bioRxiv preprint 14 these nucleotides either due to the predicted base pairing or steric hindrance (figure 2B). The', 'cite_spans': [], 'ref_spans': [], 'section': '144'}, {'text': 'NMIA reactivity for the interacting nucleotides in the stem-loops with downstream residues of 302 PK 1, 2 and 3 again largely agreed with the predicted structure, although the SHAPE data 303 suggests that there might be fewer interactions than previously predicted. However, differences 304 here could be due to heterogeneity in the formation of PKs in this experiment. The evidence 305 for loop-downstream interaction was weaker for PK4. The copyright holder for this preprint (which was not peer-reviewed) is the . https://doi.org/10.1101/2020.01.10.901801 doi: bioRxiv preprint', 'cite_spans': [{'start': 187, 'end': 190, 'text': '303', 'ref_id': None}], 'ref_spans': [], 'section': '301'}, {'text': 'orientation. 351 Since removal of all four PKs resulted in a significant decrease in replication, the minimal 352 requirements to maintain wt levels of replication were investigated. As near wt level of 353 replication was observed when only one PK was present, all further mutagenesis was 354 performed in a C11 replicon plasmid containing only PK 1. In addition, the orientation of PK 1 was reversed by \"flipping\" the nucleotide sequence to 367 potentially facilitate hybridisation of the loop with upstream rather than downstream sequences.', 'cite_spans': [{'start': 13, 'end': 16, 'text': '351', 'ref_id': None}], 'ref_spans': [], 'section': 'Function of the PKs in replication is dependent on downstream interactions and 350'}, {'text': 'Changing the orientation of the PK reduced replicon replication to a similar level seen in the replication decreased until at passage three there is a 2.5 fold reduction compared to that of 398 author/funder. All rights reserved. No reuse allowed without permission.', 'cite_spans': [], 'ref_spans': [], 'section': '368'}, {'text': 'The copyright holder for this preprint (which was not peer-reviewed) is the . https://doi.org/10.1101/2020.01.10.901801 doi: bioRxiv preprint passage 0 (figure 5B). Therefore, it appears that replicons with a single PK are at a competitive 399 disadvantage compared to those with two or more. The copyright holder for this preprint (which was not peer-reviewed) is the . https://doi.org/10.1101/2020.01.10.901801 doi: bioRxiv preprint 20 of infectious virus despite being able to replicate after transfection into cells, is consistent with 448 a requirement for RNA structure within the PK region being required for virus assembly. The 5′ UTR of FMDV is unique amongst picornaviruses due to its large size and the presence 454 of multiple RNA elements, some of which still have unknown function. One of these features 455 is a series of repeated PKs varying in number from 2-4, depending on virus strain. In this study, 456 we sequentially deleted or mutated the PKs to help understand their role in the viral life cycle. 457 We also confirmed the predicted PK structures by SHAPE mapping, although there may be Although all viruses isolated to date contain at least two PKs, replicons or viruses containing a 464 single PK were still replication competent. However, replicons with more than a single PK 465 were found to have a competitive advantage over replicons with a single PK when sequentially 466 passaged. Replicons lacking all PKs displayed poor passaging potential even when co-467 transfected with yeast tRNA, reinforcing the observation of a significant impact in replication.', 'cite_spans': [{'start': 920, 'end': 923, 'text': '456', 'ref_id': None}, {'start': 1022, 'end': 1025, 'text': '457', 'ref_id': None}], 'ref_spans': [], 'section': '368'}, {'text': 'Moreover, viruses recovered from genomes with reduced numbers of PKs were slower growing 469 and produced smaller plaques. In addition, these differences were more pronounced in more PKs is functionally competent as no differences was seen between replicons congaing a single 472 author/funder. All rights reserved. No reuse allowed without permission.', 'cite_spans': [], 'ref_spans': [], 'section': '468'}, {'text': 'The copyright holder for this preprint (which was not peer-reviewed) is the . https://doi.org/10.1101/2020.01.10.901801 doi: bioRxiv preprint 21 copy of PK1 or PK4. This observation is consistent with a previous report of deletion of PK1, 473 along with the Poly-C-tract, with no adverse effect in viral replication (37). This also supports 474 our findings that the truncation of the Poly-C-tract to create the C11 construct had no effect on 475 replicon replication in the cell lines tested. As has been described with Mengo virus, it is 476 possible that the role of the poly-C-tract is essential in other aspects of the viral lifecycle which 477 cannot be recapitulated in a standard tissue culture system (39).', 'cite_spans': [{'start': 443, 'end': 446, 'text': '475', 'ref_id': None}], 'ref_spans': [], 'section': '468'}, {'text': 'The presence of at least two PKs in all viral isolates sequenced so far suggests that multiple 480 PKs confer a competitive advantage in replication. Here we showed by sequential passage that 481 replicons containing at least two PKs were maintained at a level similar to wt, but replicons 482 containing only one PK showed a persistent decline. It is unclear why some viral isolates 483 contain two, three or four PKs is still unknown, but this may be stochastic variation or may 484 reflect subtle effects of host range or geographical localisation. The copyright holder for this preprint (which was not peer-reviewed) is the . https://doi.org/10.1101/2020.01.10.901801 doi: bioRxiv preprint author/funder. All rights reserved. No reuse allowed without permission.', 'cite_spans': [], 'ref_spans': [], 'section': '479'}, {'text': 'The copyright holder for this preprint (which was not peer-reviewed) is the . https://doi.org/10.1101/2020.01.10.901801 doi: bioRxiv preprint The copyright holder for this preprint (which was not peer-reviewed) is the . https://doi.org/10.1101/2020.01.10.901801 doi: bioRxiv preprint Significance is shown comparing the replication of C11 PK disrupt and C11 PK restore (Aii). Significance shown is compared to wt replicon. Error bars are calculated by SEM, n = 3, * P 673 < 0.05, **** P < 0.0001. 674 author/funder. All rights reserved. No reuse allowed without permission.', 'cite_spans': [], 'ref_spans': [], 'section': '479'}, {'text': 'The copyright holder for this preprint (which was not peer-reviewed) is the . https://doi.org/10.1101/2020.01.10.901801 doi: bioRxiv preprint 33 675 author/funder. All rights reserved. No reuse allowed without permission.', 'cite_spans': [], 'ref_spans': [], 'section': '479'}, {'text': 'The copyright holder for this preprint (which was not peer-reviewed) is the . https://doi.org/10.1101/2020.01.10.901801 doi: bioRxiv preprint ', 'cite_spans': [], 'ref_spans': [], 'section': '479'}], 'bib_entries': {'BIBREF0': {'ref_id': 'b0', 'title': 'Genetic economy in 598 picornaviruses: Foot-and-mouth disease virus replication exploits alternative precursor 599 cleavage pathways', 'authors': [{'first': 'T', 'middle': [], 'last': 'Jackson', 'suffix': ''}, {'first': 'T', 'middle': ['J'], 'last': 'Tuthill', 'suffix': ''}, {'first': 'D', 'middle': ['J'], 'last': 'Rowlands', 'suffix': ''}, {'first': 'N', 'middle': ['J'], 'last': 'Stonehouse', 'suffix': ''}], 'year': 2017, 'venue': 'PLOS Pathog', 'volume': '13', 'issn': '', 'pages': '', 'other_ids': {}}, 'BIBREF2': {'ref_id': 'b2', 'title': 'A universal protocol to 602 generate consensus level genome sequences for foot-and-mouth disease virus and other 603 positive-sense polyadenylated RNA viruses using the Illumina MiSeq', 'authors': [{'first': 'N', 'middle': ['D'], 'last': 'Sanderson', 'suffix': ''}, {'first': 'N', 'middle': ['J'], 'last': 'Knowles', 'suffix': ''}, {'first': 'D', 'middle': ['P'], 'last': 'King', 'suffix': ''}, {'first': 'E', 'middle': ['M'], 'last': 'Cottam', 'suffix': ''}], 'year': 2014, 'venue': 'BMC Genomics', 'volume': '604', 'issn': '', 'pages': '', 'other_ids': {}}, 'BIBREF3': {'ref_id': 'b3', 'title': 'Library preparation for highly accurate population 606 sequencing of RNA viruses', 'authors': [{'first': 'A', 'middle': [], 'last': 'Acevedo', 'suffix': ''}, {'first': 'R', 'middle': [], 'last': 'Andino', 'suffix': ''}], 'year': 2014, 'venue': 'Nat Protoc', 'volume': '9', 'issn': '', 'pages': '1760--1769', 'other_ids': {}}, 'BIBREF4': {'ref_id': 'b4', 'title': 'IDBA-UD: a de novo assembler for 608 single-cell and metagenomic sequencing data with highly uneven depth', 'authors': [{'first': 'Y', 'middle': [], 'last': 'Peng', 'suffix': ''}, {'first': 'Hcm', 'middle': [], 'last': 'Leung', 'suffix': ''}, {'first': 'S', 'middle': ['M'], 'last': 'Yiu', 'suffix': ''}, {'first': 'Fyl', 'middle': [], 'last': 'Chin', 'suffix': ''}], 'year': 2012, 'venue': '', 'volume': '', 'issn': '', 'pages': '', 'other_ids': {}}, 'BIBREF6': {'ref_id': 'b6', 'title': 'Basic local alignment 611 search tool', 'authors': [{'first': 'S', 'middle': ['F'], 'last': 'Altschul', 'suffix': ''}, {'first': 'W', 'middle': [], 'last': 'Gish', 'suffix': ''}, {'first': 'W', 'middle': [], 'last': 'Miller', 'suffix': ''}, {'first': 'E', 'middle': ['W'], 'last': 'Myers', 'suffix': ''}, {'first': 'D', 'middle': ['J'], 'last': 'Lipman', 'suffix': ''}], 'year': 1990, 'venue': 'J Mol Biol', 'volume': '215', 'issn': '', 'pages': '403--410', 'other_ids': {}}, 'BIBREF7': {'ref_id': 'b7', 'title': 'Genetically engineered foot-and-613 mouth disease viruses with poly(C) tracts of two nucleotides are virulent in mice', 'authors': [{'first': 'E', 'middle': [], 'last': 'Rieder', 'suffix': ''}, {'first': 'T', 'middle': [], 'last': 'Bunch', 'suffix': ''}, {'first': 'F', 'middle': [], 'last': 'Brown', 'suffix': ''}, {'first': 'P', 'middle': ['W'], 'last': 'Mason', 'suffix': ''}], 'year': 1993, 'venue': 'J 614 Virol', 'volume': '67', 'issn': '', 'pages': '5139--5184', 'other_ids': {}}, 'BIBREF9': {'ref_id': 'b9', 'title': 'Both cis and trans Activities of Foot-and-Mouth Disease Virus 617 3D Polymerase Are Essential for Viral RNA Replication', 'authors': [{'first': 'N', 'middle': ['J'], 'last': 'Stonehouse', 'suffix': ''}], 'year': 2016, 'venue': 'J Virol', 'volume': '90', 'issn': '', 'pages': '6864--6883', 'other_ids': {}}, 'BIBREF10': {'ref_id': 'b10', 'title': 'Mutational analysis of the 619 mengovirus poly(C) tract and surrounding heteropolymeric sequences', 'authors': [{'first': 'L', 'middle': [], 'last': 'Martin', 'suffix': ''}, {'first': 'G', 'middle': [], 'last': 'Duke', 'suffix': ''}, {'first': 'J', 'middle': [], 'last': 'Osorio', 'suffix': ''}, {'first': 'D', 'middle': [], 'last': 'Hall', 'suffix': ''}, {'first': 'A', 'middle': [], 'last': 'Palmenberg', 'suffix': ''}], 'year': 1996, 'venue': 'J Virol', 'volume': '620', 'issn': '', 'pages': '2027--2031', 'other_ids': {}}, 'BIBREF11': {'ref_id': 'b11', 'title': 'No reuse allowed without permission. The copyright holder for this preprint (which was not peer-reviewed) is the', 'authors': [], 'year': None, 'venue': '', 'volume': '', 'issn': '', 'pages': '', 'other_ids': {'DOI': ['10.1101/2020.01.10.901801']}}, 'BIBREF12': {'ref_id': 'b12', 'title': 'Figure 3. The poly-C-tract is dispensable and only one PK is required for wt replication', 'authors': [], 'year': None, 'venue': '', 'volume': '', 'issn': '', 'pages': '', 'other_ids': {}}, 'BIBREF13': {'ref_id': 'b13', 'title': 'A replicon 650 with entire poly-C-tract removed (C0) was transfected alongside wt, 3D-GNN and C11 651 replicons into BHK-21 cells (B). Replicons with sequentially deleted PKs (ΔPK 34, ΔPK 234 652 and C11 ΔPK 1234) were assayed for replication in BHK', 'authors': [{'first': '3d', 'middle': [], 'last': 'Wt', 'suffix': ''}], 'year': None, 'venue': '', 'volume': '', 'issn': '', 'pages': '', 'other_ids': {}}, 'BIBREF14': {'ref_id': 'b14', 'title': 'All replication assays were measured by counting the number of GFP 655 author/funder. All rights reserved. No reuse allowed without permission. The copyright holder for this preprint (which was not peer-reviewed) is the', 'authors': [], 'year': None, 'venue': 'Replication of replicon with PK 4 as the sole remaining PK (C11 PK 4)', 'volume': '', 'issn': '', 'pages': '', 'other_ids': {'DOI': ['10.1101/2020.01.10.901801']}}, 'BIBREF15': {'ref_id': 'b15', 'title': 'Error bars shown are calculated by SEM, n = 3', 'authors': [], 'year': None, 'venue': '', 'volume': '', 'issn': '', 'pages': '', 'other_ids': {}}, 'BIBREF17': {'ref_id': 'b17', 'title': 'No reuse allowed without permission. The copyright holder for this preprint (which was not peer-reviewed) is the', 'authors': [], 'year': None, 'venue': '', 'volume': '', 'issn': '', 'pages': '', 'other_ids': {'DOI': ['10.1101/2020.01.10.901801']}}}, 'ref_entries': {'FIGREF0': {'text': 'and-mouth disease virus (FMDV) is a single stranded positive sense RNA virus of the 45 genus Aphthovirus in the family Picornaviridae. It occurs as seven, antigenically diverse 46 serotypes; A, O, C, Asia 1, South African Territories (SAT) 1, 2 and 3. It is the causative agent 47 of foot-and-mouth disease (FMD), a highly contagious disease of cloven-hooved animals 48 affecting most notably cattle, pigs, sheep and goats in addition to wild species such as the 49 African buffalo. Disease outbreaks have serious economic implications resulting from trade 50 restrictions, reduced productivity and the slaughter of infected and at-risk animals (1). The 51 2001 outbreak in the UK caused economic losses of over £8 billion to the tourism and 52 agricultural sectors. Inactivated virus vaccines are used in countries in which FMD is endemic, 53 but these are often strain-specific and provide little cross protection between serotypes (2). 54 Antigenic variation together with the relatively short duration of immunity following 55 vaccination combine to complicate control of the disease (3). In addition, the carrier state, in 56 which asymptomatically infected animals continue to shed virus, contributes to the spread of 57 FMDV (4). An improved understanding of the viral life cycle may be important for the 58 development of improved vaccines and other control measures. 59 60 The FMDV genome (approximately 8.4 kb) consists of a single open reading frame flanked by 61 5′ and 3′ untranslated regions (UTRs) (figure 1A) (5). The translated region encodes both 62 structural and non-structural proteins. The P1 region encodes the capsid structural proteins VP1, 63', 'latex': None, 'type': 'figure'}, 'FIGREF1': {'text': '73 large poly-C tract of variable length (which can be up to 200 nt), a region containing two to 74 four tandemly repeated pseudoknots (PKs), the cis acting replication element (cre) and the 75 internal ribosome entry site (IRES) (5, 11, 12). Of these five structural domains, functions have 76 been ascribed to only two, the cre and IRES. The cre region is involved in uridylation of the 77 RNA primer peptide, VPg (also known as 3B), and the IRES determines the initiation of 78 translation of the viral polyprotein (13, 14). The roles of the S-fragment, the poly-C tract and 79 the PKs in viral replication are not fully elucidated, however recent studies have shown that 80 truncations to the S-fragment can play key roles in the innate immune response to viral 81 infection (15-17). It has also recently been reported that viruses containing a deletion within 82 the pseudoknot region showed an attenuated phenotype in bovine cell lines while remaining 83 unchanged in porcine, suggesting a role for the pseudoknots in viral tropism (18).', 'latex': None, 'type': 'figure'}, 'FIGREF3': {'text': '138 1311 as forward and reverse primers respectively. The PCR product was digested with SpeI 139 and KpnI before ligation into a NheI and KpnI digested wt pRep ptGFP replicon. Sequences of 140 all primers are available upon request.141 142', 'latex': None, 'type': 'figure'}, 'FIGREF5': {'text': 'prepared as above and a sample (12 pmol) was heated to 95 o C for 2 minutes before 151 cooling on ice. RNA folding buffer (100 mM HEPES, 66 mM MgCl2 and 100 mM NaCl) and 152 RNase Out (Invitrogen) was added to the RNA and incubated at 37 o C for 30 minutes. Once 153 folded, RNA was treated with NMIA compound at a final concentration of 5 mM or DMSO as 154 a negative control for 50 minutes at 37 o C. Following incubation, labelled RNA was ethanol 155 precipitated and resuspended in 10 μl 0.5 x TE buffer.', 'latex': None, 'type': 'figure'}, 'FIGREF6': {'text': '10 minutes in a thermocycler. A reverse transcription master mix containing 4 μl first 161 strand buffer, 1 μl 100 mM DTT, 0.5 μl RNase Out, 1 μl Supsercript III (Invitrogen), 1 μl 10 162 mM PCR dNTP mix (Promega) and 0.5 μl RNase free water, was then added to the 163 RNA/primer complex and extension carried out by incubation at 52 o C for 30 minutes.', 'latex': None, 'type': 'figure'}, 'FIGREF7': {'text': ', cDNA:RNA hybrids were disassociated by incubation with 1 μl 4M NaOH at 166 95 o C for 3 minutes before neutralisation with 2 μl 2M HCl. Extended cDNA was ethanol 167 precipitated and resuspended in 40 μl deionized formamide (Thermo Fisher). Sequencing 168 ladders were made similarly using 6 pmol of RNA with the inclusion of 1 μl 10 mM ddCTP in 169 the reverse transcription mix and using a differentially labelled fluorescent primer (either Hex 170 or FAM). 20 μl of sequencing ladder was combined with NMIA or DMSO samples and 171 dispatched on dry ice for capillary electrophoresis (Dundee DNA seq).', 'latex': None, 'type': 'figure'}, 'FIGREF8': {'text': 'was analysed using QuShape and reactivity overlaid onto the RNA 174 structure using VARNA (29, 30).', 'latex': None, 'type': 'figure'}, 'FIGREF9': {'text': 'in all cell lines was assessed in 24-well plates with 0.5 µg/cm 2 of RNA 178 using Lipofectin transfection reagent (Life Technologies) as previously described (28). For 179 complementation assays, BHK-21 cells seeded into 24-well plates were allowed to adhere for 180 16 hours before transfection with 1 µg of replicon RNA using Lipofectin. Each transfection 181 was performed in duplicate and experiments were biologically repeated. Replicon replication by live cell imaging using an IncuCyte Zoom Dual colour FLR, an automated 183 phase-contrast and fluorescence microscope within a humidifying incubator. At hourly 184 intervals up to 24 hours post transfection, images of each well were taken and used to count 185 the number of ptGFP positive cells per well.', 'latex': None, 'type': 'figure'}, 'FIGREF10': {'text': 'competition assays was performed by co-transfecting BHK-21 cells with in vitro 188 transcribed replicon RNA and harvesting total cell RNA at 8 hours post transfection using 189 TRIzol reagent (Thermo Fisher Scientific). The harvested RNA was then purified using the 190 Direct-zol RNA MiniPrep kit (Zymo Research) with on-column DNase I treatment and eluted 191 in DEPC treated water. The purified passaged RNA (1 µg) was transfected onto the naïve BHKhere are based on plasmid T7S3 which encodes a full length infectious copy 196 of FMDV O1 Kaufbeuren (31). The reporter was removed from replicons by digestion with 197 PsiI and XmaI restriction enzymes and replaced with the corresponding fragment from pT7S3 198 encoding the capsid proteins. Full length viral RNA was transcribed using a T7 MEGAscript 199 kit (Thermo Fisher Scientific), DNase treated using TurboDNase (Thermo Fisher Scientific) 200 and purified using a MEGAclear Transcription Clean-Up kit (Thermo Fisher Scientific). 201 RNA quality and concentration were determined by denaturing agarose gel electrophoresis 202 and Qubit RNA BR Assay Kit (Thermo Fisher Scientific).', 'latex': None, 'type': 'figure'}, 'FIGREF11': {'text': '-transfection cell lysates were freeze-thawed and clarified by centrifugation.208    Clarified lysate was blind passaged onto naïve BHK-21 cells, this was continued for five 209 rounds of passaging.210 211 Sequencing of recovered virus. 212 Recovered viruses at passage 4, were sequenced using an Illumina Miseq (illumine) using a 213 modified version of a previously described PCR-free protocol ((32, 33)). Total RNA was 214 extracted from clarified passage 4 lysates using TRizol reagent (Thermo Fisher Scientific) 215 and residual genomic DNA removed using DNA-free DNA removal Kit (Thermo Fisher 216 Scientific). RNA was precipitated using 3 M sodium acetate and ethanol, 10 ul of purified 217 RNA (containing 1 pg to 5 µg) of RNA was used in a reverse transcription reaction as 218 previously described (33, 34). Following reverse transcription cDNA was purified and 219 quantified using a Qubit ds DNA HS Assay kit (Thermo Fisher Scientific) and a cDNA 220 library prepared using Nextera XT DNA Sample Preparation Kit (Illumina). Sequencing was 221 carried out on the MiSeq platform using MiSeq Reagent Kit v2 (300 cycles) chemistry 222 (Illumina).', 'latex': None, 'type': 'figure'}, 'FIGREF13': {'text': 'of recovered virus. 232 Confluent BHK-21 cell monolayers were infected with 10-fold serial dilutions of virus stock, 233 overlaid with Eagle overlay media supplemented with 5 % tryptose phosphate broth solution 234 (Sigma Aldrich), penicillin (100 units/ml and streptomycin (100 µg/ml) (Sigma Aldrich) and 235 0.6 % Indubiose (MP Biomedicals) and incubated for 48 hours at 37 o C. Cells were fixed and 236 stained with 1 % (w/v) methylene blue in 10 % (v/v) ethanol and 4 % formaldehyde in PBS. 237 238 Fixed plaques were scanned and images measured using a GNU Image Manipulation 239 Program IMP (GIMP, available at https://www.gimp.org). For each plaque, horizontal and 240 vertical diameter in pixels was taken and an average of these two values was calculated. All 241 plaques per well were measured. 242 243 Cell killing assays. 244 Virus titre was determined by plaque assays. BHK-21 cells were seeded with 3 x10 4 245 cells/well in 96 well plates and allowed to settle overnight. Cell monolayers were inoculated 246 with each rescued virus at MOI of 0.01 PFU for 1 hour, inoculum was removed and 150 µl of 247 fresh GMEM (supplemented with 1 % FCS) was added to each well. Appearance of CPE was 248 monitored every 30 minutes using the IncuCyte S3.', 'latex': None, 'type': 'figure'}, 'FIGREF14': {'text': 'protein 3A, and a goat anti-Mouse IgG (H+L) highly cross-adsorbed 254 secondary antibody, Alexa Fluor 488 (Life Technologies). Each transcript was transfected in 255 triplicate and the experiment biologically repeated three times. BHK-21 cells were seeded 256 into T25 flasks 16 hours prior to transfection with 10 µg RNA. The transfection mix was left 257 on the cells for 1 hour before the media was changed to VGM (Glasgow Minimum Essential 258 Medium (Sigma-Aldrich), 1% Foetal Bovine Serum -Brazil origin (Life Science Production) 259 and 5% Tryptose Phosphate Broth (Sigma-Aldrich).', 'latex': None, 'type': 'figure'}, 'FIGREF15': {'text': 'After a further 3 hours, cells were dissociated using trypsin-EDTA 0.05% phenol red (Life261 Technologies), pelleted at 200 g for 3 minutes and fixed in 4% paraformaldehyde for 40 262 minutes. Cells were then transferred to a 96-well u-bottom plate and pelleted; this and all 263 subsequent pelleting steps were done at 300 xg for 5 minutes. Cells were resuspended in 0.5% 264 BSA in PBS blocking buffer (Melford), pelleted and resuspended in 1/1000 2C2 antibody and 265 left shaking at 500 rpm at 4 o C for 14 hours in an Eppendorf Thermomixer C plate shaker. The 266 cells were pelleted and subsequently resuspended in blocking buffer three times to wash, 267 resuspended in 1/200 anti-mouse fluorescent secondary antibody and rotated at 500 rpm at 268 24 o C for 1 hour before washing a final three times. Cells were then resuspended in 500 µl PBS 269 and data were collected on the LSR Fortessa (BD Biosciences) using BD FACSDivaTM 270 software. Data were exported as flow cytometry standard (FCS) files, and were analysed in 271 FlowJo 10 using the gating strategy shown in Figure 7.', 'latex': None, 'type': 'figure'}, 'FIGREF16': {'text': 'of PKs was initially predicted in 1987 by computational and visual analysis of 279 the 5′ UTR sequence (12). The prediction of the presence of multiple PKs was strengthened by 280 the observation that variation in the length of this region between different virus isolates 281 equated to the gain or loss of PK-length sequence units. However, the definitive demonstration 282 of PK structure remains a challenge. Here, we used selective 2′ hydroxyl acylation analysed by 283 primer extension (SHAPE) to investigate the secondary structure of the PK region.', 'latex': None, 'type': 'figure'}, 'FIGREF17': {'text': 'representing FMDV UTRs were folded prior to treatment with NMIA, a 286 compound that forms 2′-O-adducts when interacting with non-paired nucleotides, or DMSO as 287 a negative control. Labelled RNAs were purified and used as templates in reverse transcription 288 reactions using fluorescently labelled primers. Elongation of the reverse transcription products 289 terminates at adducts, resulting in cDNA fragments of different lengths, which were analysed 290 by gel electrophoresis alongside a sequencing ladder to identify sites of NMIA interaction. The 291 whole PK region was surprisingly reactive suggesting that it was largely single stranded or 292 highly flexible (figure 2A). To investigate if the SHAPE data agreed with the predicted 293structure, the NMIA reactivity was overlaid onto the previous PK structure prediction (12).', 'latex': None, 'type': 'figure'}, 'FIGREF19': {'text': '296 being unreactive, suggestive of base-pairing. Formation of the stem of PK4 was less convincing, 297 although the stem nucleotides still had relatively low reactivity in agreement with the other PK 298 models. For all the PKs, the nucleotides in the loop regions and the predicted downstream299', 'latex': None, 'type': 'figure'}, 'FIGREF20': {'text': 'the NMIA reactivities with the original predicted structure the SHAPE data were 308 compatible to the PK models and potentially shed new light on the requirements of the loop 309 interactions.', 'latex': None, 'type': 'figure'}, 'FIGREF21': {'text': 'PK is sufficient for efficient replication.312    The replicon system was based on the O1K FMDV sequence which includes four similar but 313 non-identical PKs (figure 1). The PKs were sequentially deleted from the 3′ side (i.e. PK 4-PK 314 1), and replication of the resulting modified replicons assessed.', 'latex': None, 'type': 'figure'}, 'FIGREF22': {'text': 'complete removal of all PKs, an AflII site was inserted into the ptGFP replicon 317 plasmid which resulted in reduction of the poly-C-tract to 11 cytosine residues. This C11 318 replicon was investigated alongside a wt replicon and one with lethal polymerase mutations 319 (3D-GNN). These controls were used to confirm that truncation of the poly-C tract had no 320 measurable effect on replication the two cell lines tested, as previously reported (37) (figure 321 3A). For completeness, we further removed the entire poly-C-tract (C0) and showed that this 322 had no observable negative effect on replication of the replicon (figure 3B). The C11 construct 323 was then used as the \"backbone\" for removal of all four PKs.', 'latex': None, 'type': 'figure'}, 'FIGREF23': {'text': 'measuring ptGFP reporter expression, in parallel with transfection of a wt and 327 3D-GNN replicon, where the 3D-GNN replicon is used to monitor ptGFP expression resulting 328 from translation of input RNA in the absence of replication. Reporter expression was recorded 329 using an IncuCyte Zoom automatic fluorescent microscope and is shown at 8 hours post-', 'latex': None, 'type': 'figure'}, 'FIGREF25': {'text': '∆PK 234 respectively) replicated at similar levels to the wt replicon (figure 3C-D). 334 However, a replicon containing no PKs (C11 ∆PK 1234) showed a significant (~ 4 fold) 335 reduction in replication in BHK-21 cells compared to the wt C11 replicon. A larger reduction 336 in replication (28 fold) was seen in the MDBK cell line, supporting previous publications on 337 the potential role in host cell tropism (18). Replication of the C11 ∆PK 1234 replicon in MDBK 338 cells was however still significantly above that of the 3D-GNN negative control. These data 339 suggest that although the PKs are not essential for replication at least one PK is required for wt 340 levels of replication. 341 342In the experiments above PK1 was the sole remaining PK and we therefore investigated 343 whether other PKs could similarly support wt replication. We deleted all the PKs to create the 344 C11 construct and re-inserted PK4 as the only PK (C11 PK4). Near wt levels of replication 345 were observed following transfection into both cell types suggesting that there is no functional 346 difference between PK1 and PK4 (figure 3E).', 'latex': None, 'type': 'figure'}, 'FIGREF27': {'text': 'to interrupt base pairing and abrogate formation of the PK structure were 357 made in the loop of PK 1 and the corresponding downstream nucleotides. The substitutions 358 (shown in red) created a GAGA motif both in the loop and downstream regions and reduced 359 the replication of the mutated replicon (C11 PK disrupt) equivalent to that of the replicon 360 containing no PKs, thereby supporting the predicted structure (figure 4A). Base pairing 361 potential was then restored by mutation of the relevant nucleotides in the loop and downstream 362 region to GGGG and CCCC respectively. Restoring the interaction using an alternate sequence 363 increased replication significantly compared to the disrupted PK replicon (~ 4 fold), although 364 this was still slightly below that of the wt (~ 0.7 fold decrease) (figure 4A).', 'latex': None, 'type': 'figure'}, 'FIGREF29': {'text': '369absence of PKs (figure 4B). This suggests that the role of the PKs in genome replication is 370 dependent on both sequence, structure and orientation.', 'latex': None, 'type': 'figure'}, 'FIGREF30': {'text': 'studies above suggested that removal of up to three of the four PKs present in the 375 wt sequence had no clear effect on replicon replication, although deletion of all four was 376 significantly detrimental. To investigate whether multiple PKs conferred more subtle 377 advantages for replication than were evident from single round transfection experiments we 378 carried out sequential passages of replicon RNA following transfection of the PK deleted forms 379 in competition with a wt replicon. Different reporter genes (ptGFP or mCherry) were used to 380 distinguish the competing replicons.', 'latex': None, 'type': 'figure'}, 'FIGREF31': {'text': 'ptGFP; wt, ∆PK 34, ∆PK 234 and C11 ∆PK 1234 were co-transfected into 383 BHK-21 cells together with either a wt mCherry replicon or yeast tRNA as a control. The 384 replication of each of the co-transfected replicons was compared by observing ptGFP and 385 mCherry expression over three sequential passages. Passaging was achieved by harvesting total 386 RNA using Trizol-reagent 8 hours post-transfection. Harvested RNA was purified and then re-387 transfected into naïve BHK-21 cells.', 'latex': None, 'type': 'figure'}, 'FIGREF32': {'text': 'transfection of the wt, ∆PK 34 or ∆PK 234 with yeast tRNA as controls showed no 390 differences in replication as expected (Figure 5A). Likewise, when PK mutants were co-391 transfected with a wt replicon after three passages, the number of green fluorescent cells 392 produced by the ∆PK 34 replicon was comparable to that of the wt, suggesting no competitive 393 advantage of four PKs over two. For both, there was a reduction in replication after the first 394 passage but recovery to near that of the original transfection by the third passage. However, 395 when co-transfected with the wt replicon, the ∆PK 234 replicon showed a similar drop in 396 replication in passage two, but showed no subsequent recovery following each passage and397', 'latex': None, 'type': 'figure'}, 'FIGREF33': {'text': 'transfection with the wt mCherry replicon reduced the replication of the C11 ∆PK 1234 402 replicon to background levels as seen when comparing to the yeast tRNA control. By passage 403 two the ptGFP signal of the C11 ∆PK 1234 was no longer detectable, suggesting that this 404 replicon has been out competed (figure 5C). Although the initial replication of C11 ∆PK 1234 405 was greater when co-transfected with yeast tRNA than when in competition with wt mCherry 406 replicon, the ptGFP signal was reduced at passage two and was at background level by passage 407 three (figure 5C). Replication of the mCherry wt replicon was not influenced by co-transfection 408 with the ptGFP constructs (figure 5D), as expected. Together these data suggest that the minor 409 replicative advantage conferred by multiple PKs are quickly compounded over multiple 410 replication cycles to provide a replicative advantage.', 'latex': None, 'type': 'figure'}, 'FIGREF34': {'text': 'a PK is essential for the production of infectious virus413    As replicons lacking all PKs could replicate and replicons with reduced numbers of PKs414 appeared to be at a competitive disadvantage compared to the wt construct, we investigated the 415 consequences of PK manipulation on the complete viral life cycle. The ∆PK 34, ∆PK 234 and 416 C11 ∆PK 1234 mutations were introduced into an FMDV infectious clone by replacement of 417 sequence encoding ptGFP with that encoding the O1K structural proteins. RNA transcripts 418 were transfected into BHK-21 cells alongside a wt O1K viral transcript and blind passaged 5 419 times by transferring the cell supernatant at 24 hours post transfection onto naïve BHK-21 cells.', 'latex': None, 'type': 'figure'}, 'FIGREF36': {'text': '427 rate of CPE (figure 6A) and plaque size (figure 6B-C) of ∆PK 34 and ∆PK 234 when compared 428 to the wt O1K virus. Rate of CPE was monitored by infecting BHK-21 cells with a known MOI 429 (0.01) of recovered virus, cells were then monitored for signs of CPE (shown as a decrease in 430 cell confluency) as measured by an automated imaging platform (Incucyte Zoom). Both ∆PK 431 34 and ∆PK 234 showed delayed onset of CPE with ∆PK 34 being the slowest, initial CPE 432 occurring at approximately 39 hours and 29 hours post infection respectively, compared to the 433 22 hours seen in the wt control. This mirrored plaque assay data where ∆PK 34 displayed a 434 significantly smaller plaque phenotype when compared to the wt control (average of 13.8 pixels 435 compared to 37.4), the slower rate of CPE seen in ∆PK 234 made a small, but not significant 436 difference (average 31.9 pixels).', 'latex': None, 'type': 'figure'}, 'FIGREF37': {'text': '∆PK 1234 produced no infectious virus the ability of the full-length genome lacking 439 PKs to replicate was investigated. BHK-21 cells were transfected with the same RNA 440 transcripts as above alongside additional controls, mock-transfected and transfected with wt 441 and treated with 3 mM GuHCl (a replication inhibitor) as negative controls. Six hours post-442 transfection, cells were harvested, fixed and labelled with an anti-3A antibody and fluorescent 443 secondary antibody. Cells were then analysed using flow cytometry and anti-3A antibody 444 signal used as an indirect measure of genome replication (figure 7). The results were similar to 445 those of the replicon experiments and showed that all the modified virus genomes were able to 446 undergo robust replication. The inability of the C11 ∆PK 1234 genome to support production 447 author/funder. All rights reserved. No reuse allowed without permission.', 'latex': None, 'type': 'figure'}, 'FIGREF39': {'text': '458fewer strong interactions maintaining the PKs than was previously predicted. This may indicate 459 high conformational flexibility of this region of the genome. SHAPE mapping was also 460 supported by mutation of predicted key interactions between nucleotides in the loop and 461 downstream, disruption of which reduced replication to that of the C11 ∆PK 1234 replicon.', 'latex': None, 'type': 'figure'}, 'FIGREF41': {'text': '470 relevant cells lines (i.e. in MDBK cells compared to BHK 21 cells). It is likely that each of the 471', 'latex': None, 'type': 'figure'}, 'FIGREF42': {'text': 'although removal of all four PKs resulted in a significant decrease in replicon and 487 viral genome replication, replication was not abolished, showing that PKs are not essential to 488 support genome replication. However, deletion of all PKs from an infectious clone completely 489 abolished the ability to recover infectious virus. This suggests that the genome lacking all PKs 490 is defective in a function associated with virion assembly and is compatible with our evidence 491 for the presence of a packaging signal in a similar location on the genome to PK1 (22). It is 492 possible that structural flexibility at this site in the genome allows the RNA to adopt alternate 493 conformations with different roles in genome replication and virion assembly. A functional 494 requirement for multiple RNA conformations may explain the relatively weak interactions 495 between nucleotides involved in stabilising the PK motif as observed by SHAPE analysis or 496 by structural prediction.', 'latex': None, 'type': 'figure'}, 'FIGREF43': {'text': 'was supported by funding from the Biotechnology and Biological Sciences Research 503 Council (BBSRC) of the United Kingdom (research grant BB/K003801/1). Additionally, the 504 Pirbright Institute receives grant-aided support from the BBSRC (projects BB/E/I/00007035, 505 BB/E/I/00007036 and BBS/E/I/00007037).', 'latex': None, 'type': 'figure'}, 'FIGREF44': {'text': 'Jones TJD, Rushton J. 2013. The economic impacts of foot and mouth disease 511 -What are they, how big are they and where do they occur? Prev Vet Med 112:161-M, Parida S. 2018. Foot and mouth disease vaccine strain selection: current 514 approaches and future perspectives. Expert Rev Vaccines 17:577-591.', 'latex': None, 'type': 'figure'}, 'FIGREF45': {'text': 'J-H. 2013. Requirements for improved vaccines against foot-and-mouth disease 516 epidemics. Clin Exp Vaccine Res 2:8-18.', 'latex': None, 'type': 'figure'}, 'FIGREF46': {'text': 'C, Eschbaumer M, Rekant SI, Pacheco JM, Smoliga GR, Hartwig EJ, 518 Rodriguez LL, Arzt J. 2016. The Foot-and-Mouth Disease Carrier State Divergence in 519 Cattle. J Virol 90:6344-64.', 'latex': None, 'type': 'figure'}, 'FIGREF47': {'text': \"C, Tulman ER, Delhon G, Lu Z, Carreno A, Vagnozzi A, Kutish GF, Rock 521 DL. 2005. Comparative genomics of foot-and-mouth disease virus. J Virol 79:6487-D. 1997. Dissecting the roles of VP0 cleavage and RNA packaging in 525 picornavirus capsid stabilization: the structure of empty capsids of foot-and-mouth 526 disease virus. J Virol 71:9743-52. 527 7. Gao Y, Sun S-Q, Guo H-C. 2016. Biological function of Foot-and-mouth disease virus 528 non-structural proteins and non-coding elements. Virol J 13:107. 529 8. Herod MR, Gold S, LaseckaDykes L, Wright C, Ward JC, McLean TC, Forrest S, 530 Jackson T, Tuthill TJ, Rowlands DJ, Stonehouse NJ. 2017. Genetic economy in 531 picornaviruses: Foot-and-mouth disease virus replication exploits alternative precursor 532 cleavage pathway. PLOS Pathog 13:e1006666. 533 9. Tulloch F, Pathania U, Luke GA, Nicholson J, Stonehouse NJ, Rowlands DJ, Jackson 534 T, Tuthill T, Haas J, Lamond AI, Ryan MD. 2014. FMDV replicons encoding green 535 fluorescent protein are replication competent. J Virol Methods 209:35-40. 536 10. Herod MR, Tulloch F, Loundras E-A, Ward JC, Rowlands DJ, Stonehouse NJ. 2015. 537 Employing transposon mutagenesis to investigate foot-and-mouth disease virus 538 replication. J Gen Virol 96:3507-3518. 539 11. Mellor EJC, Brown F, Harris TJR. 1985. Analysis of the Secondary Structure of the 540 Poly(C) Tract in Foot-and-Mouth Disease Virus RNAs. J Gen Virol 66:1919-1929. 541 12. Clarke BE, Brown AL, Currey KM, Newton SE, Rowlands DJ, Carroll AR. 1987. 542 Potential secondary and tertiary structure in the genomic RNA of foot and mouth 543 disease virus. Nucleic Acids Res 15:7067-7079. 544 13. Nayak A, Goodfellow IG, Woolaway KE, Birtley J, Curry S, Belsham GJ. 2006. Role 545 of RNA structure and RNA binding activity of foot-and-mouth disease virus 3C 546 protein in VPg uridylylation and virus replication. J Virol 80:9865-75. Kloc A, Diaz-San Segundo F, Schafer EA, Rai DK, Kenney M, de los Santos T, 555 Rieder E. 2017. Foot-and-mouth disease virus 5'-terminal S fragment is required for 556 replication and modulation of the innate immune response in host cells. Virology 557 512:132-143. 558 17. Kloc A, Rai DK, Rieder E. 2018. The roles of picornavirus untranslated regions in 559 infection and innate immunity. Front Microbiol. Frontiers Media S.A. 560 18. Zhu Z, Yang F, Cao W, Liu H, Zhang K, Tian H, Dang W, He J, Guo J, Liu X, Zheng 561 H. 2019. The Pseudoknot Region of the 5' Untranslated Region Is a Determinant of 562 Viral Tropism and Virulence of Foot-and-Mouth Disease Virus. J Virol 93. 563 19. Mohapatra JK, Pawar SS, Tosh C, Subramaniam S, Palsamy R, Sanyal A, Hemadri D, 564 Pattnaik B. 2011. Genetic characterization of vaccine and field strains of serotype A 565 foot-andmouth disease virus from India. Acta Virol 55:349-352. 566 20. Escarmís C, Dopazo J, Dávila M, Palma EL, Domingo E. 1995. Large deletions in the 567 5'-untranslated region of foot-and-mouth disease virus of serotype C. Virus Res 568 35:155-67.\", 'latex': None, 'type': 'figure'}, 'FIGREF48': {'text': 'Carocci M, Bakkali-Kassimi L. 2012. The encephalomyocarditis virus. Virulence', 'latex': None, 'type': 'figure'}, 'FIGREF49': {'text': \". Wutz G, Auer H, Nowotny N, Grosse B, Skern T, Kuechler E. 1996. Equine rhinovirus Xrn1 produce a pathogenic Dengue virus RNA. Elife 3. 576 24. Kieft JS, Rabe JL, Chapman EG. 2015. New hypotheses derived from the structure of 577 a flaviviral Xrn1-resistant RNA: Conservation, folding, and host adaptation. RNA Biol 578 12:1169-77.579 25. Gultyaev AP, Olsthoorn RCL. 2010. A family of non-classical pseudoknots in 580 influenza A and B viruses. RNA Biol 7:125-9. 581 26. Moss WN, Dela-Moss LI, Priore SF, Turner DH. 2012. The influenza A segment 7 582 mRNA 3' splice site pseudoknot/hairpin family. RNA Biol 9:1305-10. 583 27. Plant EP, Dinman JD. 2008. The role of programmed-1 ribosomal frameshifting in 584 coronavirus propagation. Front Biosci 13:4873-81. 585 28. Herod MR, Ferrer-Orta C, Loundras E-A, Ward JC, Verdaguer N, Rowlands DJ, 586 Stonehouse NJ. 2016. Both cis and trans Activities of Foot-and-Mouth Disease Virus 587 3D Polymerase Are Essential for Viral RNA Replication. J Virol 90:6864-6883. 588 29. Karabiber F, McGinnis JL, Favorov O V., Weeks KM. 2013. QuShape: Rapid, 589 accurate, and best-practices quantification of nucleic acid probing information, 590 resolved by capillary electrophoresis. RNA 19:63-73. 591 30. Darty K, Denise A, Ponty Y. 2009. VARNA: Interactive drawing and editing of the 592 RNA secondary structure. Bioinformatics 25:1974-1975. 593 31. King AMQ, Blakemore WE, Ellard FM, Drew J, Stuart DI. 1999. Evidence for the role 594 of His-142 of protein 1C in the acid-induced disassembly of foot-and-mouth disease 595 virus capsids. J Gen Virol 80:1911-1918. 596 32. Herod MR, Gold S, Lasecka-Dykes L, Wright C, Ward JC, McLean TC, Forrest S, 597 author/funder. All rights reserved. No reuse allowed without permission.\", 'latex': None, 'type': 'figure'}, 'FIGREF50': {'text': \"Replicon and PK schematic. Schematic of the FMDV O1K sub-genomic replicon, 627 showing both 5' and 3' untranslated regions (UTRs) together with the RNA structures present 628 in these regions. IRES-driven translation produces a single polyprotein. Here, the structural 629 proteins have been replaced with a green fluorescent reporter, upstream of the non-structural 630 proteins 2A-3D (A). Predicted PK structures, with putative interactions highlighted in hot-pink 631 are shown. Numbers indicate nucleotide positions after the poly-C-tract (B). Sequence 632 alignment of the 4 PKs, with the interacting regions shown in hot-pink and invariant 633 nucleotides represented by asterisk (C).\", 'latex': None, 'type': 'figure'}, 'FIGREF51': {'text': 'SHAPE NMIA reactivity of the PK region. NMIA reactivity at nucleotides 640 following the poly-C-tract (PCT). High reactivity indicates increased chance of the nucleotide 641 being non base-paired at that position (A). NMIA reactivity of each PK overlaid onto the 642 predicted PK structure using VARNA (30). Loop and downstream interactions represent those 643 supported by SHAPE data (B). NMIA reactivity is represented on a colour scale from low 644 (white) to high (red) (n = 4). 645 646 author/funder. All rights reserved. No reuse allowed without permission.', 'latex': None, 'type': 'figure'}, 'FIGREF52': {'text': \"Disrupting the PK structure and reversing the orientation of a PK reduces 663 replication. Cartoon representations of disrupting and restoring mutations made to PK 1, 664 where nucleotides in the bulge of the stem loop and interacting region downstream were 665 mutated to disrupt structure formation 'PK disrupt', or mutated to maintain bulge and 666 downstream interaction but with different nucleotides 'PK restore' (Ai). Replication of PK 667 disrupt and restore mutants were measured by transfection of RNA into BHK-21 cells and 668 shown here at 8 hours post-transfection alongside wt, 3D-GNN and C11 ΔPK 1234 controls.\", 'latex': None, 'type': 'figure'}, 'FIGREF54': {'text': 'Visual representation of the reversing of the nucleotide sequence of PK1 creating the C11 PK 671 Rvs construct (Bi). Replication of PK Rvs at 8 hours post transfection of BHK-21 cells (Bii).', 'latex': None, 'type': 'figure'}, 'FIGREF56': {'text': 'More than 2 PKs provides a replicative advantage in co-transfection', 'latex': None, 'type': 'figure'}, 'TABREF0': {'text': 'encephalomyocarditis virus (EMCV) and equine rhinitis A virus (ERAV) (21, 22). However, in both cases the PKs are located at the 5′ side of the poly-C-tract, making their location in the FMDV genome unique. PKs have been reported to have roles in several aspects of viral replication including splicing 97 (e.g. HIV and influenza), ribosomal frameshifting (e.g. coronaviruses) and RNase protection 98 (e.g. Dengue virus) (23-27). In the work reported here, the role of the PKs in the FMDV life cycle was investigated, together with biochemical probing of PK structures. The combination of both virus and replicon systems allowed us to distinguish effects on genome replication and 101 other aspects of the viral life cycle. Selective mutation within the PK domain and sequential 102 deletion of PKs confirmed the importance of PK structure and that although genome replication', 'latex': None, 'type': 'table'}, 'TABREF1': {'text': \"Materials and MethodsCells lines.BHK-21 cells obtained from the ATCC (LGC Standard) were maintained in Dulbecco's modified Eagle's Medium with glutamine (Sigma-Aldrich) supplemented with 10 % foetal calf serum (FCS), 50 U/ml penicillin and 50 µg/ml streptomycin.\", 'latex': None, 'type': 'table'}, 'TABREF2': {'text': 'Wt, C11, ∆PK 34 and ∆PK 234 constructs all resulted in the production of infectious virus as was expected from the replicon experiments, with no alteration to input sequence. However, the C11 ∆PK 1234, which replicated (albeit to a lesser degree) as a replicon, produced no recoverable infectious virus(Table 1). Interestingly, there were differences noted in both the', 'latex': None, 'type': 'table'}}, 'back_matter': [{'text': 'author/funder. All rights reserved. No reuse allowed without permission.The copyright holder for this preprint (which was not peer-reviewed) is the The copyright holder for this preprint (which was not peer-reviewed) is the . https://doi.org/10.1101/2020.01.10.901801 doi: bioRxiv preprint', 'cite_spans': [], 'ref_spans': [], 'section': 'annex'}]}\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    with client_hdfs.read(fpaths_json[i]) as reader:\n",
    "        from json import load\n",
    "        print(load(reader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['paper_id', 'metadata', 'abstract', 'body_text', 'bib_entries', 'ref_entries', 'back_matter'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = {}\n",
    "for i in range(1):\n",
    "    with client_hdfs.read(fpaths_json[i]) as reader:\n",
    "        from json import load\n",
    "        articles[i] = load(reader)\n",
    "\n",
    "articles[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above, we can see that there are keys that describe each paper of the biorxiv journal. Of these, Our paritcular interest lies in the abstract and the body_text. The title is useful too, and to get it, we have to look inside the metadata part of the file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['title', 'authors'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[0]['metadata'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If printed as a whole, each file would look like the below output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'> {'title': 'The RNA pseudoknots in foot-and-mouth disease virus are dispensable for genome replication but essential for the production of infectious virus. 2 3', 'authors': [{'first': 'Joseph', 'middle': ['C'], 'last': 'Ward', 'suffix': '', 'affiliation': {}, 'email': ''}, {'first': 'Lidia', 'middle': [], 'last': 'Lasecka-Dykes', 'suffix': '', 'affiliation': {}, 'email': ''}, {'first': 'Chris', 'middle': [], 'last': 'Neil', 'suffix': '', 'affiliation': {}, 'email': ''}, {'first': 'Oluwapelumi', 'middle': [], 'last': 'Adeyemi', 'suffix': '', 'affiliation': {}, 'email': ''}, {'first': 'Sarah', 'middle': [], 'last': '', 'suffix': '', 'affiliation': {}, 'email': ''}, {'first': '', 'middle': [], 'last': 'Gold', 'suffix': '', 'affiliation': {}, 'email': ''}, {'first': 'Niall', 'middle': [], 'last': 'Mclean', 'suffix': '', 'affiliation': {}, 'email': ''}, {'first': 'Caroline', 'middle': [], 'last': 'Wright', 'suffix': '', 'affiliation': {}, 'email': ''}, {'first': 'Morgan', 'middle': ['R'], 'last': 'Herod', 'suffix': '', 'affiliation': {}, 'email': ''}, {'first': 'David', 'middle': [], 'last': 'Kealy', 'suffix': '', 'affiliation': {}, 'email': ''}, {'first': 'Emma', 'middle': [], 'last': '', 'suffix': '', 'affiliation': {}, 'email': ''}, {'first': 'Warner', 'middle': [], 'last': '', 'suffix': '', 'affiliation': {}, 'email': ''}, {'first': 'Donald', 'middle': ['P'], 'last': 'King', 'suffix': '', 'affiliation': {}, 'email': ''}, {'first': 'Tobias', 'middle': ['J'], 'last': 'Tuthill', 'suffix': '', 'affiliation': {}, 'email': ''}, {'first': 'David', 'middle': ['J'], 'last': 'Rowlands', 'suffix': '', 'affiliation': {}, 'email': ''}, {'first': 'Nicola', 'middle': ['J'], 'last': '', 'suffix': '', 'affiliation': {}, 'email': ''}, {'first': 'Stonehouse', 'middle': [], 'last': 'A#', 'suffix': '', 'affiliation': {}, 'email': ''}]}\n"
     ]
    }
   ],
   "source": [
    "with client_hdfs.read(f'{fpaths_json[0]}', encoding = 'utf-8') as reader:\n",
    "    f = json.load(reader)\n",
    "    print(type(f),f['metadata'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paper_id\n",
      "metadata\n",
      "abstract\n",
      "body_text\n",
      "bib_entries\n",
      "ref_entries\n",
      "back_matter\n"
     ]
    }
   ],
   "source": [
    "with client_hdfs.read(f'{fpaths_json[0]}', encoding = 'utf-8') as reader:\n",
    "    f = json.load(reader)\n",
    "    for line in f:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to load and read a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(filename):\n",
    "    with open(filename +  \".json\") as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading all files from local disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our dataset is not prohibitively large(7-8GB) it is easilty possible to reado it from disk as shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to read all json files in the directory in one go (Cite: [Kernel on Kaggle](https://www.kaggle.com/maksimeren/covid-19-literature-clustering))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "all_json = glob.glob(f'/s/chopin/b/grad/sanketm/cord19dataset/**/*.json', recursive=True)\n",
    "end_time = datetime.now()\n",
    "print(f'Loaded the paths of {len(all_json)} files from disk. Took {end_time-start_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_json[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_125json = all_json[0:12500]\n",
    "_125json[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpaths_json[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a class so that we could refer to the loaded articles easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Article:\n",
    "    \n",
    "    def __init__(self,filepath):\n",
    "        with client_hdfs.read(filepath,encoding='utf=8') as f:\n",
    "            metadata = json.load(f)\n",
    "            self.paper_id = metadata['paper_id']\n",
    "            self.title = metadata['metadata']['title']\n",
    "            self.abstract = \"\"\n",
    "            self.body_text = \"\"\n",
    "            \n",
    "            #Abstract\n",
    "            try:\n",
    "                for entry in metadata['abstract']:\n",
    "                    self.abstract += str(entry['text'])\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "            \n",
    "\n",
    "            #body_text\n",
    "            for entry in metadata['body_text']:\n",
    "                self.body_text += str(entry['text'])\n",
    "\n",
    "\n",
    "            \n",
    "    def __repr__(self):\n",
    "        return f'Article Object with id: {self.paper_id} \\n title:{self.title} \\n abstract:{self.abstract}'\n",
    "            \n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article Object with id: 0015023cc06b5362d332b3baf348d11567ca2fbb \n",
      " title:The RNA pseudoknots in foot-and-mouth disease virus are dispensable for genome replication but essential for the production of infectious virus. 2 3 \n",
      " abstract:word count: 194 22 Text word count: 5168 23 24 25 author/funder. All rights reserved. No reuse allowed without permission. Abstract 27 The positive stranded RNA genomes of picornaviruses comprise a single large open reading 28 frame flanked by 5′ and 3′ untranslated regions (UTRs). Foot-and-mouth disease virus (FMDV) 29 has an unusually large 5′ UTR (1.3 kb) containing five structural domains. These include the 30 internal ribosome entry site (IRES), which facilitates initiation of translation, and the cis-acting 31 replication element (cre). Less well characterised structures are a 5′ terminal 360 nucleotide 32 stem-loop, a variable length poly-C-tract of approximately 100-200 nucleotides and a series of 33 two to four tandemly repeated pseudoknots (PKs). We investigated the structures of the PKs 34 by selective 2′ hydroxyl acetylation analysed by primer extension (SHAPE) analysis and 35 determined their contribution to genome replication by mutation and deletion experiments. 36 SHAPE and mutation experiments confirmed the importance of the previously predicted PK 37 structures for their function. Deletion experiments showed that although PKs are not essential 38for replication, they provide genomes with a competitive advantage. However, although 39 replicons and full-length genomes lacking all PKs were replication competent, no infectious 40 virus was rescued from genomes containing less than one PK copy. This is consistent with our 41 earlier report describing the presence of putative packaging signals in the PK region. 42 43 author/funder. All rights reserved. No reuse allowed without permission. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Article Object with id: 00340eea543336d54adda18236424de6a5e91c9d \n",
      " title:Analysis Title: Regaining perspective on SARS-CoV-2 molecular tracing and its implications \n",
      " abstract:During the past three months, a new coronavirus (SARS-CoV-2) epidemic has been growing exponentially, affecting over 100 thousand people worldwide, and causing enormous distress to economies and societies of affected countries. A plethora of analyses based on viral sequences has already been published, in scientific journals as well as through non-peer reviewed channels, to investigate SARS-CoV-2 genetic heterogeneity and spatiotemporal dissemination. We examined all full genome sequences currently available to assess the presence of sufficient information for reliable phylogenetic and phylogeographic studies. Our analysis clearly shows severe limitations in the present data, in light of which any finding should be considered, at the very best, preliminary and hypothesis-generating. Hence the need for avoiding stigmatization based on partial information, and for continuing concerted efforts to increase number and quality of the sequences required for robust tracing of the epidemic.. CC-BY-NC-ND 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity.is the (which was not peer-reviewed) The copyright holder for this preprint . \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "articles = {}\n",
    "for i in range(2):\n",
    "        print(Article(fpaths_json[i]),\"\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = {}\n",
    "for i in range(2):\n",
    "        articles[i] = Article(fpaths_json[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'word count: 194 22 Text word count: 5168 23 24 25 author/funder. All rights reserved. No reuse allowed without permission. Abstract 27 The positive stranded RNA genomes of picornaviruses comprise a single large open reading 28 frame flanked by 5′ and 3′ untranslated regions (UTRs). Foot-and-mouth disease virus (FMDV) 29 has an unusually large 5′ UTR (1.3 kb) containing five structural domains. These include the 30 internal ribosome entry site (IRES), which facilitates initiation of translation, and the cis-acting 31 replication element (cre). Less well characterised structures are a 5′ terminal 360 nucleotide 32 stem-loop, a variable length poly-C-tract of approximately 100-200 nucleotides and a series of 33 two to four tandemly repeated pseudoknots (PKs). We investigated the structures of the PKs 34 by selective 2′ hydroxyl acetylation analysed by primer extension (SHAPE) analysis and 35 determined their contribution to genome replication by mutation and deletion experiments. 36 SHAPE and mutation experiments confirmed the importance of the previously predicted PK 37 structures for their function. Deletion experiments showed that although PKs are not essential 38for replication, they provide genomes with a competitive advantage. However, although 39 replicons and full-length genomes lacking all PKs were replication competent, no infectious 40 virus was rescued from genomes containing less than one PK copy. This is consistent with our 41 earlier report describing the presence of putative packaging signals in the PK region. 42 43 author/funder. All rights reserved. No reuse allowed without permission.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[0].abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Article Object with id: 00340eea543336d54adda18236424de6a5e91c9d \n",
       " title:Analysis Title: Regaining perspective on SARS-CoV-2 molecular tracing and its implications \n",
       " abstract:During the past three months, a new coronavirus (SARS-CoV-2) epidemic has been growing exponentially, affecting over 100 thousand people worldwide, and causing enormous distress to economies and societies of affected countries. A plethora of analyses based on viral sequences has already been published, in scientific journals as well as through non-peer reviewed channels, to investigate SARS-CoV-2 genetic heterogeneity and spatiotemporal dissemination. We examined all full genome sequences currently available to assess the presence of sufficient information for reliable phylogenetic and phylogeographic studies. Our analysis clearly shows severe limitations in the present data, in light of which any finding should be considered, at the very best, preliminary and hypothesis-generating. Hence the need for avoiding stigmatization based on partial information, and for continuing concerted efforts to increase number and quality of the sequences required for robust tracing of the epidemic.. CC-BY-NC-ND 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity.is the (which was not peer-reviewed) The copyright holder for this preprint ."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have explored the structure of the dataset, let us focus on our areas of interest as mentioned earlier: namely the Title, Abstract and the body of the text.\\\n",
    "We know that we can access these for a given paper by scanning the metadata for a paper of choice for its title and abstract, and if we need it, using its `sha` column value to get the full text of the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let us clean the metadata by dropping all NA valued rows for the title, sha and abstract columns.\n",
    "\n",
    "[Pandas filter rows](https://stackoverflow.com/questions/17071871/how-to-select-rows-from-a-dataframe-based-on-column-values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio= meta.loc[meta['source_x'] == 'biorxiv']\n",
    "\n",
    "print(f'Meta count:{len(meta)} biorxiv papers count: {len(bio)}')\n",
    "\n",
    "bio_clean = bio.drop_duplicates().dropna()\n",
    "\n",
    "len(bio_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above dropna condition doesn't work out,  number of rows in the resulting dataframe is zero.\n",
    "We found a solution Cite: [stack overflow post](https://stackoverflow.com/questions/39241346/pandas-dropna-on-specify-attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_clean = bio.loc[~(bio.sha.isnull())]\n",
    "\n",
    "len(bio_clean)\n",
    "\n",
    "bio_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can remove columns that are unneeded or have only nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bio_clean['source_x'].unique(),\n",
    "bio_clean['pmcid'].unique(),\n",
    "bio_clean['pubmed_id'].unique(),\n",
    "bio_clean['Microsoft Academic Paper ID'].unique(),\n",
    "bio_clean['WHO #Covidence'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_slim = bio_clean.drop(['Microsoft Academic Paper ID','WHO #Covidence','pubmed_id','pmcid'], axis = 1)\n",
    "\n",
    "bio_slim = bio_slim.drop(['url','doi','license','full_text_file'],axis =1 )\n",
    "\n",
    "bio_slim = bio_slim[~bio_slim.has_pdf_parse.isin(['False'])]\n",
    "\n",
    "bio_slim =bio_slim.loc[~(bio_slim.abstract.isnull())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we had seen at the send of section 2.3, `fpaths_json` contains the paths of all the json files(i.e. the articles) in it.\n",
    "Now we should load this into a dataframe for it to be queryable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index: 0 of 12500\n",
      "Processing index: 1250 of 12500\n",
      "Processing index: 2500 of 12500\n",
      "Processing index: 3750 of 12500\n",
      "Processing index: 5000 of 12500\n",
      "Processing index: 6250 of 12500\n",
      "Processing index: 7500 of 12500\n",
      "Processing index: 8750 of 12500\n",
      "Processing index: 10000 of 12500\n",
      "Processing index: 11250 of 12500\n",
      " time taken : 0:06:44.018415\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "dict_ = {'paper_id': [], 'abstract': [], 'body_text': [], 'authors': [], 'title': []}\n",
    "for idx, entry in enumerate(_125hdfsjson):\n",
    "    if idx % (len(_125hdfsjson) // 10) == 0:\n",
    "        print(f'Processing index: {idx} of {len(_125hdfsjson)}')\n",
    "    content = Article(entry)\n",
    "    \n",
    "    # get metadata information\n",
    "    meta_data = meta.loc[meta['sha'] == content.paper_id]\n",
    "    # no metadata, skip this paper\n",
    "    if len(meta_data) == 0:\n",
    "        continue\n",
    "    \n",
    "\n",
    "    dict_['body_text'].append(content.body_text)\n",
    "    \n",
    "       \n",
    "    # get metadata information\n",
    "    meta_data = meta.loc[meta['sha'] == content.paper_id]\n",
    "    \n",
    "    try:\n",
    "        # if more than one author\n",
    "        authors = meta_data['authors'].values[0].split(';')\n",
    "        if len(authors) > 2:\n",
    "            # more than 2 authors, may be problem when plotting, so take first 2 append with ...\n",
    "            dict_['authors'].append(\". \".join(authors[:2]) + \"...\")\n",
    "        else:\n",
    "            # authors will fit in plot\n",
    "            dict_['authors'].append(\". \".join(authors))\n",
    "    except Exception as e:\n",
    "        # if only one author - or Null valie\n",
    "        dict_['authors'].append(meta_data['authors'].values[0])\n",
    "    \n",
    "    # add the title information, add breaks when needed\n",
    "    try:\n",
    "        title = meta_data['title'].values[0], 40\n",
    "        dict_['title'].append(title)\n",
    "    # if title was not provided\n",
    "    except Exception as e:\n",
    "        dict_['title'].append(meta_data['title'].values[0])\n",
    "    \n",
    "       \n",
    "    dict_['paper_id'].append(meta_data['sha'].values[0])\n",
    "    dict_['abstract'].append(meta_data['abstract'].values[0])\n",
    "    \n",
    "df_covid = pd.DataFrame(dict_, columns=['paper_id', 'abstract', 'body_text', 'authors', 'title'])\n",
    "df_covid.head()\n",
    "end_time = datetime.now()\n",
    "print(f' time taken : {(end_time  - start_time)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10298 10298 10298 10298 10298\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "194"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(dict_['paper_id']),len(dict_['abstract']),len(dict_['body_text']),len(dict_['authors']),len(dict_['title']))\n",
    "\n",
    "len(df_covid['abstract'][0].strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>abstract</th>\n",
       "      <th>body_text</th>\n",
       "      <th>authors</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract_word_count</th>\n",
       "      <th>body_word_count</th>\n",
       "      <th>body_unique_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0015023cc06b5362d332b3baf348d11567ca2fbb</td>\n",
       "      <td>AbstractThe positive stranded RNA genomes of p...</td>\n",
       "      <td>VP3, and VP0 (which is further processed to VP...</td>\n",
       "      <td>Joseph C. Ward.  Lidia Lasecka-Dykes...</td>\n",
       "      <td>(The RNA pseudoknots in foot-and-mouth disease...</td>\n",
       "      <td>194</td>\n",
       "      <td>1709</td>\n",
       "      <td>704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00340eea543336d54adda18236424de6a5e91c9d</td>\n",
       "      <td>During the past three months, a new coronaviru...</td>\n",
       "      <td>In December 2019, a novel coronavirus, SARS-Co...</td>\n",
       "      <td>Carla Mavian.  Simone Marini...</td>\n",
       "      <td>(Regaining perspective on SARS-CoV-2 molecular...</td>\n",
       "      <td>138</td>\n",
       "      <td>2487</td>\n",
       "      <td>1032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>004f0f8bb66cf446678dc13cf2701feec4f36d76</td>\n",
       "      <td>We integrate the human movement and healthcare...</td>\n",
       "      <td>The 2019-nCoV epidemic has spread across China...</td>\n",
       "      <td>Hanchu Zhou.  Jianan Yang...</td>\n",
       "      <td>(Healthcare-resource-adjusted vulnerabilities ...</td>\n",
       "      <td>34</td>\n",
       "      <td>749</td>\n",
       "      <td>378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00911cf4f99a3d5ae5e5b787675646a743574496</td>\n",
       "      <td>ABSTRARCTThe fast accumulation of viral metage...</td>\n",
       "      <td>Metagenomic sequencing, which allows us to dir...</td>\n",
       "      <td>Jiayu Shang.  Yanni Sun</td>\n",
       "      <td>(CHEER: hierarCHical taxonomic classification ...</td>\n",
       "      <td>139</td>\n",
       "      <td>5153</td>\n",
       "      <td>1371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00d16927588fb04d4be0e6b269fc02f0d3c2aa7b</td>\n",
       "      <td>AbstractInfectious bronchitis (IB) causes sign...</td>\n",
       "      <td>Infectious bronchitis (IB), which is caused by...</td>\n",
       "      <td>Salman L. Butt.  Eric C. Erwood...</td>\n",
       "      <td>(Real-time, MinION-based, amplicon sequencing ...</td>\n",
       "      <td>248</td>\n",
       "      <td>3958</td>\n",
       "      <td>1199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00eb9220dc8cd351393b6b035323d350f103f8c6</td>\n",
       "      <td>Importance: As with other traumatic events, pa...</td>\n",
       "      <td>Evidence from prior pandemics suggests that, a...</td>\n",
       "      <td>Victor M. Castro.  Roy H Perlis</td>\n",
       "      <td>(Impact of COVID-19 on psychiatric assessment ...</td>\n",
       "      <td>246</td>\n",
       "      <td>2103</td>\n",
       "      <td>759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0139ea4ca580af99b602c6435368e7fdbefacb03</td>\n",
       "      <td>AbstractBackgroundNipah Virus (NiV) came into ...</td>\n",
       "      <td>Nipah is an infectious negative-sense single-s...</td>\n",
       "      <td>Nishi Kumari.  Ayush Upadhyay...</td>\n",
       "      <td>(A Combined Evidence Approach to Prioritize Ni...</td>\n",
       "      <td>326</td>\n",
       "      <td>2382</td>\n",
       "      <td>1034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>013d9d1cba8a54d5d3718c229b812d7cf91b6c89</td>\n",
       "      <td>Background: A novel coronavirus (2019-nCoV) em...</td>\n",
       "      <td>In December 2019, a cluster of patients with p...</td>\n",
       "      <td>Shengjie Lai.  Isaac Bogoch...</td>\n",
       "      <td>(Assessing spread risk of Wuhan novel coronavi...</td>\n",
       "      <td>292</td>\n",
       "      <td>4596</td>\n",
       "      <td>1205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>018fb5e62fbbcae07d57d94d29ac630dcc4dccf9</td>\n",
       "      <td>Introduction: Recent events highlight how emer...</td>\n",
       "      <td>Recent events highlight how emerging and re-em...</td>\n",
       "      <td>DAVIDE GORI.  Erik Boetto...</td>\n",
       "      <td>(Analysis of the scientific literature in the ...</td>\n",
       "      <td>200</td>\n",
       "      <td>2906</td>\n",
       "      <td>1131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>01d162d7fae6aaba8e6e60e563ef4c2fca7b0e18</td>\n",
       "      <td>Faced with the current large-scale public heal...</td>\n",
       "      <td>The sudden outbreak of the new coronavirus (SA...</td>\n",
       "      <td>Xiaoyang Ji.  Chunming Zhang...</td>\n",
       "      <td>(TWIRLS, an automated topic-wise inference met...</td>\n",
       "      <td>223</td>\n",
       "      <td>4386</td>\n",
       "      <td>1309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   paper_id  \\\n",
       "0  0015023cc06b5362d332b3baf348d11567ca2fbb   \n",
       "1  00340eea543336d54adda18236424de6a5e91c9d   \n",
       "2  004f0f8bb66cf446678dc13cf2701feec4f36d76   \n",
       "3  00911cf4f99a3d5ae5e5b787675646a743574496   \n",
       "4  00d16927588fb04d4be0e6b269fc02f0d3c2aa7b   \n",
       "5  00eb9220dc8cd351393b6b035323d350f103f8c6   \n",
       "6  0139ea4ca580af99b602c6435368e7fdbefacb03   \n",
       "7  013d9d1cba8a54d5d3718c229b812d7cf91b6c89   \n",
       "8  018fb5e62fbbcae07d57d94d29ac630dcc4dccf9   \n",
       "9  01d162d7fae6aaba8e6e60e563ef4c2fca7b0e18   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  AbstractThe positive stranded RNA genomes of p...   \n",
       "1  During the past three months, a new coronaviru...   \n",
       "2  We integrate the human movement and healthcare...   \n",
       "3  ABSTRARCTThe fast accumulation of viral metage...   \n",
       "4  AbstractInfectious bronchitis (IB) causes sign...   \n",
       "5  Importance: As with other traumatic events, pa...   \n",
       "6  AbstractBackgroundNipah Virus (NiV) came into ...   \n",
       "7  Background: A novel coronavirus (2019-nCoV) em...   \n",
       "8  Introduction: Recent events highlight how emer...   \n",
       "9  Faced with the current large-scale public heal...   \n",
       "\n",
       "                                           body_text  \\\n",
       "0  VP3, and VP0 (which is further processed to VP...   \n",
       "1  In December 2019, a novel coronavirus, SARS-Co...   \n",
       "2  The 2019-nCoV epidemic has spread across China...   \n",
       "3  Metagenomic sequencing, which allows us to dir...   \n",
       "4  Infectious bronchitis (IB), which is caused by...   \n",
       "5  Evidence from prior pandemics suggests that, a...   \n",
       "6  Nipah is an infectious negative-sense single-s...   \n",
       "7  In December 2019, a cluster of patients with p...   \n",
       "8  Recent events highlight how emerging and re-em...   \n",
       "9  The sudden outbreak of the new coronavirus (SA...   \n",
       "\n",
       "                                   authors  \\\n",
       "0  Joseph C. Ward.  Lidia Lasecka-Dykes...   \n",
       "1          Carla Mavian.  Simone Marini...   \n",
       "2             Hanchu Zhou.  Jianan Yang...   \n",
       "3                  Jiayu Shang.  Yanni Sun   \n",
       "4       Salman L. Butt.  Eric C. Erwood...   \n",
       "5          Victor M. Castro.  Roy H Perlis   \n",
       "6         Nishi Kumari.  Ayush Upadhyay...   \n",
       "7           Shengjie Lai.  Isaac Bogoch...   \n",
       "8             DAVIDE GORI.  Erik Boetto...   \n",
       "9          Xiaoyang Ji.  Chunming Zhang...   \n",
       "\n",
       "                                               title  abstract_word_count  \\\n",
       "0  (The RNA pseudoknots in foot-and-mouth disease...                  194   \n",
       "1  (Regaining perspective on SARS-CoV-2 molecular...                  138   \n",
       "2  (Healthcare-resource-adjusted vulnerabilities ...                   34   \n",
       "3  (CHEER: hierarCHical taxonomic classification ...                  139   \n",
       "4  (Real-time, MinION-based, amplicon sequencing ...                  248   \n",
       "5  (Impact of COVID-19 on psychiatric assessment ...                  246   \n",
       "6  (A Combined Evidence Approach to Prioritize Ni...                  326   \n",
       "7  (Assessing spread risk of Wuhan novel coronavi...                  292   \n",
       "8  (Analysis of the scientific literature in the ...                  200   \n",
       "9  (TWIRLS, an automated topic-wise inference met...                  223   \n",
       "\n",
       "   body_word_count  body_unique_words  \n",
       "0             1709                704  \n",
       "1             2487               1032  \n",
       "2              749                378  \n",
       "3             5153               1371  \n",
       "4             3958               1199  \n",
       "5             2103                759  \n",
       "6             2382               1034  \n",
       "7             4596               1205  \n",
       "8             2906               1131  \n",
       "9             4386               1309  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_covid['abstract_word_count'] = df_covid['abstract'].apply(lambda x: len(str(x).strip().split()))\n",
    "df_covid['body_word_count'] = df_covid['body_text'].apply(lambda x: len(str(x).strip().split()))\n",
    "df_covid['body_unique_words']=df_covid['body_text'].apply(lambda x:len(set(str(x).split())))  # number of unique words in body\n",
    "df_covid.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid.to_csv(\"./df_covid.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset statistics <a id='Statistics'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalnumberofwords_abstract=df_covid['abstract_word_count'].sum()\n",
    "avg_abstract_len = totalnumberofwords_abstract/len(df_covid['abstract_word_count'])\n",
    "print(f'Total number of words across all abstracts = {totalnumberofwords_abstract}. Average abstract length = {avg_abstract_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalnumberofwords_body=df_covid['body_word_count'].sum()\n",
    "avg_body_len = totalnumberofwords_body/len(df_covid['body_word_count'])\n",
    "print(f'Total number of words across all body texts = {totalnumberofwords_body}. Average body length = {avg_body_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqwords = df_covid['body_unique_words'].sum()\n",
    "avg_uniq_words = uniqwords/len(df_covid['body_unique_words'])\n",
    "print(f'Total number of unique words across all body texts: {uniqwords}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot = plt.figure(figsize=(80,80))\n",
    "ax1 = plt.subplot(2,1,1)\n",
    "ax1.plot(df_covid['abstract_word_count'])\n",
    "#plt.xlim(right=2500)\n",
    "ax1.set_title(\"Abstract word-count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot = plt.figure(figsize=(100,108))\n",
    "ax2 = plt.subplot(2,1,2)\n",
    "ax2.plot(df_covid['body_word_count'])\n",
    "ax2.set_title(\"Article body-text word-count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.distplot(df_covid['body_word_count'])\n",
    "df_covid['body_word_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df_covid['body_unique_words'])\n",
    "df_covid['body_unique_words'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df_covid['abstract_word_count'])\n",
    "df_covid['abstract_word_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid['abstract'].describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between the unique count and the total count means that either there are some duplicate abstracts or there are blank abstracts that are being counted as the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid['body_text'].describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between the unique count and the total count means that there are some duplicate articles present in the dataset. This coulde be possible if the authors had sent their papers to different journals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid.drop_duplicates(['abstract'],inplace=True)\n",
    "\n",
    "df_covid['abstract'].describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid.drop_duplicates(['body_text'],inplace=True)\n",
    "\n",
    "df_covid['body_text'].describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now clean the text data so that our key words are clear and we do not have to worry about details we are not concerned about. [ref1](https://towardsdatascience.com/nlp-text-preprocessing-a-practical-guide-and-template-d80874676e79)  [ref2](https://www.geeksforgeeks.org/text-preprocessing-in-python-set-1/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly By now looking at the abstract columns, we must notice that the text scraped from these articles is not perfect.\n",
    "\n",
    "For example, many of the abstracts start with the word *Abstract* glued to the beginning of the first word of the real abstract body. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning dirty abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>abstract</th>\n",
       "      <th>body_text</th>\n",
       "      <th>authors</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract_word_count</th>\n",
       "      <th>body_word_count</th>\n",
       "      <th>body_unique_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0015023cc06b5362d332b3baf348d11567ca2fbb</td>\n",
       "      <td>The positive stranded RNA genomes of picornavi...</td>\n",
       "      <td>VP3, and VP0 (which is further processed to VP...</td>\n",
       "      <td>Joseph C. Ward.  Lidia Lasecka-Dykes...</td>\n",
       "      <td>(The RNA pseudoknots in foot-and-mouth disease...</td>\n",
       "      <td>194</td>\n",
       "      <td>1709</td>\n",
       "      <td>704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00340eea543336d54adda18236424de6a5e91c9d</td>\n",
       "      <td>During the past three months, a new coronaviru...</td>\n",
       "      <td>In December 2019, a novel coronavirus, SARS-Co...</td>\n",
       "      <td>Carla Mavian.  Simone Marini...</td>\n",
       "      <td>(Regaining perspective on SARS-CoV-2 molecular...</td>\n",
       "      <td>138</td>\n",
       "      <td>2487</td>\n",
       "      <td>1032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>004f0f8bb66cf446678dc13cf2701feec4f36d76</td>\n",
       "      <td>We integrate the human movement and healthcare...</td>\n",
       "      <td>The 2019-nCoV epidemic has spread across China...</td>\n",
       "      <td>Hanchu Zhou.  Jianan Yang...</td>\n",
       "      <td>(Healthcare-resource-adjusted vulnerabilities ...</td>\n",
       "      <td>34</td>\n",
       "      <td>749</td>\n",
       "      <td>378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00911cf4f99a3d5ae5e5b787675646a743574496</td>\n",
       "      <td>The fast accumulation of viral metagenomic dat...</td>\n",
       "      <td>Metagenomic sequencing, which allows us to dir...</td>\n",
       "      <td>Jiayu Shang.  Yanni Sun</td>\n",
       "      <td>(CHEER: hierarCHical taxonomic classification ...</td>\n",
       "      <td>139</td>\n",
       "      <td>5153</td>\n",
       "      <td>1371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00d16927588fb04d4be0e6b269fc02f0d3c2aa7b</td>\n",
       "      <td>Infectious bronchitis (IB) causes significant ...</td>\n",
       "      <td>Infectious bronchitis (IB), which is caused by...</td>\n",
       "      <td>Salman L. Butt.  Eric C. Erwood...</td>\n",
       "      <td>(Real-time, MinION-based, amplicon sequencing ...</td>\n",
       "      <td>248</td>\n",
       "      <td>3958</td>\n",
       "      <td>1199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00eb9220dc8cd351393b6b035323d350f103f8c6</td>\n",
       "      <td>Importance: As with other traumatic events, pa...</td>\n",
       "      <td>Evidence from prior pandemics suggests that, a...</td>\n",
       "      <td>Victor M. Castro.  Roy H Perlis</td>\n",
       "      <td>(Impact of COVID-19 on psychiatric assessment ...</td>\n",
       "      <td>246</td>\n",
       "      <td>2103</td>\n",
       "      <td>759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0139ea4ca580af99b602c6435368e7fdbefacb03</td>\n",
       "      <td>Nipah Virus (NiV) came into limelight recently...</td>\n",
       "      <td>Nipah is an infectious negative-sense single-s...</td>\n",
       "      <td>Nishi Kumari.  Ayush Upadhyay...</td>\n",
       "      <td>(A Combined Evidence Approach to Prioritize Ni...</td>\n",
       "      <td>326</td>\n",
       "      <td>2382</td>\n",
       "      <td>1034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>013d9d1cba8a54d5d3718c229b812d7cf91b6c89</td>\n",
       "      <td>: A novel coronavirus (2019-nCoV) emerged in W...</td>\n",
       "      <td>In December 2019, a cluster of patients with p...</td>\n",
       "      <td>Shengjie Lai.  Isaac Bogoch...</td>\n",
       "      <td>(Assessing spread risk of Wuhan novel coronavi...</td>\n",
       "      <td>292</td>\n",
       "      <td>4596</td>\n",
       "      <td>1205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>018fb5e62fbbcae07d57d94d29ac630dcc4dccf9</td>\n",
       "      <td>Introduction: Recent events highlight how emer...</td>\n",
       "      <td>Recent events highlight how emerging and re-em...</td>\n",
       "      <td>DAVIDE GORI.  Erik Boetto...</td>\n",
       "      <td>(Analysis of the scientific literature in the ...</td>\n",
       "      <td>200</td>\n",
       "      <td>2906</td>\n",
       "      <td>1131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>01d162d7fae6aaba8e6e60e563ef4c2fca7b0e18</td>\n",
       "      <td>Faced with the current large-scale public heal...</td>\n",
       "      <td>The sudden outbreak of the new coronavirus (SA...</td>\n",
       "      <td>Xiaoyang Ji.  Chunming Zhang...</td>\n",
       "      <td>(TWIRLS, an automated topic-wise inference met...</td>\n",
       "      <td>223</td>\n",
       "      <td>4386</td>\n",
       "      <td>1309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   paper_id  \\\n",
       "0  0015023cc06b5362d332b3baf348d11567ca2fbb   \n",
       "1  00340eea543336d54adda18236424de6a5e91c9d   \n",
       "2  004f0f8bb66cf446678dc13cf2701feec4f36d76   \n",
       "3  00911cf4f99a3d5ae5e5b787675646a743574496   \n",
       "4  00d16927588fb04d4be0e6b269fc02f0d3c2aa7b   \n",
       "5  00eb9220dc8cd351393b6b035323d350f103f8c6   \n",
       "6  0139ea4ca580af99b602c6435368e7fdbefacb03   \n",
       "7  013d9d1cba8a54d5d3718c229b812d7cf91b6c89   \n",
       "8  018fb5e62fbbcae07d57d94d29ac630dcc4dccf9   \n",
       "9  01d162d7fae6aaba8e6e60e563ef4c2fca7b0e18   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  The positive stranded RNA genomes of picornavi...   \n",
       "1  During the past three months, a new coronaviru...   \n",
       "2  We integrate the human movement and healthcare...   \n",
       "3  The fast accumulation of viral metagenomic dat...   \n",
       "4  Infectious bronchitis (IB) causes significant ...   \n",
       "5  Importance: As with other traumatic events, pa...   \n",
       "6  Nipah Virus (NiV) came into limelight recently...   \n",
       "7  : A novel coronavirus (2019-nCoV) emerged in W...   \n",
       "8  Introduction: Recent events highlight how emer...   \n",
       "9  Faced with the current large-scale public heal...   \n",
       "\n",
       "                                           body_text  \\\n",
       "0  VP3, and VP0 (which is further processed to VP...   \n",
       "1  In December 2019, a novel coronavirus, SARS-Co...   \n",
       "2  The 2019-nCoV epidemic has spread across China...   \n",
       "3  Metagenomic sequencing, which allows us to dir...   \n",
       "4  Infectious bronchitis (IB), which is caused by...   \n",
       "5  Evidence from prior pandemics suggests that, a...   \n",
       "6  Nipah is an infectious negative-sense single-s...   \n",
       "7  In December 2019, a cluster of patients with p...   \n",
       "8  Recent events highlight how emerging and re-em...   \n",
       "9  The sudden outbreak of the new coronavirus (SA...   \n",
       "\n",
       "                                   authors  \\\n",
       "0  Joseph C. Ward.  Lidia Lasecka-Dykes...   \n",
       "1          Carla Mavian.  Simone Marini...   \n",
       "2             Hanchu Zhou.  Jianan Yang...   \n",
       "3                  Jiayu Shang.  Yanni Sun   \n",
       "4       Salman L. Butt.  Eric C. Erwood...   \n",
       "5          Victor M. Castro.  Roy H Perlis   \n",
       "6         Nishi Kumari.  Ayush Upadhyay...   \n",
       "7           Shengjie Lai.  Isaac Bogoch...   \n",
       "8             DAVIDE GORI.  Erik Boetto...   \n",
       "9          Xiaoyang Ji.  Chunming Zhang...   \n",
       "\n",
       "                                               title  abstract_word_count  \\\n",
       "0  (The RNA pseudoknots in foot-and-mouth disease...                  194   \n",
       "1  (Regaining perspective on SARS-CoV-2 molecular...                  138   \n",
       "2  (Healthcare-resource-adjusted vulnerabilities ...                   34   \n",
       "3  (CHEER: hierarCHical taxonomic classification ...                  139   \n",
       "4  (Real-time, MinION-based, amplicon sequencing ...                  248   \n",
       "5  (Impact of COVID-19 on psychiatric assessment ...                  246   \n",
       "6  (A Combined Evidence Approach to Prioritize Ni...                  326   \n",
       "7  (Assessing spread risk of Wuhan novel coronavi...                  292   \n",
       "8  (Analysis of the scientific literature in the ...                  200   \n",
       "9  (TWIRLS, an automated topic-wise inference met...                  223   \n",
       "\n",
       "   body_word_count  body_unique_words  \n",
       "0             1709                704  \n",
       "1             2487               1032  \n",
       "2              749                378  \n",
       "3             5153               1371  \n",
       "4             3958               1199  \n",
       "5             2103                759  \n",
       "6             2382               1034  \n",
       "7             4596               1205  \n",
       "8             2906               1131  \n",
       "9             4386               1309  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_covid['abstract'] = df_covid['abstract'].apply(lambda x: str(x).replace('Abstract',''))\n",
    "df_covid['abstract'] = df_covid['abstract'].apply(lambda x: str(x).replace('abstract',''))\n",
    "df_covid['abstract'] = df_covid['abstract'].apply(lambda x: str(x).replace('ABSTRACT',''))\n",
    "df_covid['abstract'] = df_covid['abstract'].apply(lambda x: str(x).replace('ABSTRARCT',''))\n",
    "df_covid['abstract'] = df_covid['abstract'].apply(lambda x: str(x).replace('Background',''))\n",
    "df_covid.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us remove all punctuation from text and change it to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported text preprocessing libraries, time taken:0:00:00.000160\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "# import nltk \n",
    "import string \n",
    "import re \n",
    "end_time = datetime.now()\n",
    "print(f'imported text preprocessing libraries, time taken:{end_time-start_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>abstract</th>\n",
       "      <th>body_text</th>\n",
       "      <th>authors</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract_word_count</th>\n",
       "      <th>body_word_count</th>\n",
       "      <th>body_unique_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0015023cc06b5362d332b3baf348d11567ca2fbb</td>\n",
       "      <td>the positive stranded rna genomes of picornavi...</td>\n",
       "      <td>VP3, and VP0 (which is further processed to VP...</td>\n",
       "      <td>Joseph C. Ward.  Lidia Lasecka-Dykes...</td>\n",
       "      <td>(The RNA pseudoknots in foot-and-mouth disease...</td>\n",
       "      <td>194</td>\n",
       "      <td>1709</td>\n",
       "      <td>704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00340eea543336d54adda18236424de6a5e91c9d</td>\n",
       "      <td>during the past three months, a new coronaviru...</td>\n",
       "      <td>In December 2019, a novel coronavirus, SARS-Co...</td>\n",
       "      <td>Carla Mavian.  Simone Marini...</td>\n",
       "      <td>(Regaining perspective on SARS-CoV-2 molecular...</td>\n",
       "      <td>138</td>\n",
       "      <td>2487</td>\n",
       "      <td>1032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>004f0f8bb66cf446678dc13cf2701feec4f36d76</td>\n",
       "      <td>we integrate the human movement and healthcare...</td>\n",
       "      <td>The 2019-nCoV epidemic has spread across China...</td>\n",
       "      <td>Hanchu Zhou.  Jianan Yang...</td>\n",
       "      <td>(Healthcare-resource-adjusted vulnerabilities ...</td>\n",
       "      <td>34</td>\n",
       "      <td>749</td>\n",
       "      <td>378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00911cf4f99a3d5ae5e5b787675646a743574496</td>\n",
       "      <td>the fast accumulation of viral metagenomic dat...</td>\n",
       "      <td>Metagenomic sequencing, which allows us to dir...</td>\n",
       "      <td>Jiayu Shang.  Yanni Sun</td>\n",
       "      <td>(CHEER: hierarCHical taxonomic classification ...</td>\n",
       "      <td>139</td>\n",
       "      <td>5153</td>\n",
       "      <td>1371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00d16927588fb04d4be0e6b269fc02f0d3c2aa7b</td>\n",
       "      <td>infectious bronchitis (ib) causes significant ...</td>\n",
       "      <td>Infectious bronchitis (IB), which is caused by...</td>\n",
       "      <td>Salman L. Butt.  Eric C. Erwood...</td>\n",
       "      <td>(Real-time, MinION-based, amplicon sequencing ...</td>\n",
       "      <td>248</td>\n",
       "      <td>3958</td>\n",
       "      <td>1199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   paper_id  \\\n",
       "0  0015023cc06b5362d332b3baf348d11567ca2fbb   \n",
       "1  00340eea543336d54adda18236424de6a5e91c9d   \n",
       "2  004f0f8bb66cf446678dc13cf2701feec4f36d76   \n",
       "3  00911cf4f99a3d5ae5e5b787675646a743574496   \n",
       "4  00d16927588fb04d4be0e6b269fc02f0d3c2aa7b   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  the positive stranded rna genomes of picornavi...   \n",
       "1  during the past three months, a new coronaviru...   \n",
       "2  we integrate the human movement and healthcare...   \n",
       "3  the fast accumulation of viral metagenomic dat...   \n",
       "4  infectious bronchitis (ib) causes significant ...   \n",
       "\n",
       "                                           body_text  \\\n",
       "0  VP3, and VP0 (which is further processed to VP...   \n",
       "1  In December 2019, a novel coronavirus, SARS-Co...   \n",
       "2  The 2019-nCoV epidemic has spread across China...   \n",
       "3  Metagenomic sequencing, which allows us to dir...   \n",
       "4  Infectious bronchitis (IB), which is caused by...   \n",
       "\n",
       "                                   authors  \\\n",
       "0  Joseph C. Ward.  Lidia Lasecka-Dykes...   \n",
       "1          Carla Mavian.  Simone Marini...   \n",
       "2             Hanchu Zhou.  Jianan Yang...   \n",
       "3                  Jiayu Shang.  Yanni Sun   \n",
       "4       Salman L. Butt.  Eric C. Erwood...   \n",
       "\n",
       "                                               title  abstract_word_count  \\\n",
       "0  (The RNA pseudoknots in foot-and-mouth disease...                  194   \n",
       "1  (Regaining perspective on SARS-CoV-2 molecular...                  138   \n",
       "2  (Healthcare-resource-adjusted vulnerabilities ...                   34   \n",
       "3  (CHEER: hierarCHical taxonomic classification ...                  139   \n",
       "4  (Real-time, MinION-based, amplicon sequencing ...                  248   \n",
       "\n",
       "   body_word_count  body_unique_words  \n",
       "0             1709                704  \n",
       "1             2487               1032  \n",
       "2              749                378  \n",
       "3             5153               1371  \n",
       "4             3958               1199  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_covid['abstract'] = df_covid['abstract'].apply(lambda x: str(x).lower())\n",
    "df_covid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lower Case conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>abstract</th>\n",
       "      <th>body_text</th>\n",
       "      <th>authors</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract_word_count</th>\n",
       "      <th>body_word_count</th>\n",
       "      <th>body_unique_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0015023cc06b5362d332b3baf348d11567ca2fbb</td>\n",
       "      <td>the positive stranded rna genomes of picornavi...</td>\n",
       "      <td>vp3, and vp0 (which is further processed to vp...</td>\n",
       "      <td>Joseph C. Ward.  Lidia Lasecka-Dykes...</td>\n",
       "      <td>(The RNA pseudoknots in foot-and-mouth disease...</td>\n",
       "      <td>194</td>\n",
       "      <td>1709</td>\n",
       "      <td>704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00340eea543336d54adda18236424de6a5e91c9d</td>\n",
       "      <td>during the past three months, a new coronaviru...</td>\n",
       "      <td>in december 2019, a novel coronavirus, sars-co...</td>\n",
       "      <td>Carla Mavian.  Simone Marini...</td>\n",
       "      <td>(Regaining perspective on SARS-CoV-2 molecular...</td>\n",
       "      <td>138</td>\n",
       "      <td>2487</td>\n",
       "      <td>1032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>004f0f8bb66cf446678dc13cf2701feec4f36d76</td>\n",
       "      <td>we integrate the human movement and healthcare...</td>\n",
       "      <td>the 2019-ncov epidemic has spread across china...</td>\n",
       "      <td>Hanchu Zhou.  Jianan Yang...</td>\n",
       "      <td>(Healthcare-resource-adjusted vulnerabilities ...</td>\n",
       "      <td>34</td>\n",
       "      <td>749</td>\n",
       "      <td>378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00911cf4f99a3d5ae5e5b787675646a743574496</td>\n",
       "      <td>the fast accumulation of viral metagenomic dat...</td>\n",
       "      <td>metagenomic sequencing, which allows us to dir...</td>\n",
       "      <td>Jiayu Shang.  Yanni Sun</td>\n",
       "      <td>(CHEER: hierarCHical taxonomic classification ...</td>\n",
       "      <td>139</td>\n",
       "      <td>5153</td>\n",
       "      <td>1371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00d16927588fb04d4be0e6b269fc02f0d3c2aa7b</td>\n",
       "      <td>infectious bronchitis (ib) causes significant ...</td>\n",
       "      <td>infectious bronchitis (ib), which is caused by...</td>\n",
       "      <td>Salman L. Butt.  Eric C. Erwood...</td>\n",
       "      <td>(Real-time, MinION-based, amplicon sequencing ...</td>\n",
       "      <td>248</td>\n",
       "      <td>3958</td>\n",
       "      <td>1199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   paper_id  \\\n",
       "0  0015023cc06b5362d332b3baf348d11567ca2fbb   \n",
       "1  00340eea543336d54adda18236424de6a5e91c9d   \n",
       "2  004f0f8bb66cf446678dc13cf2701feec4f36d76   \n",
       "3  00911cf4f99a3d5ae5e5b787675646a743574496   \n",
       "4  00d16927588fb04d4be0e6b269fc02f0d3c2aa7b   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  the positive stranded rna genomes of picornavi...   \n",
       "1  during the past three months, a new coronaviru...   \n",
       "2  we integrate the human movement and healthcare...   \n",
       "3  the fast accumulation of viral metagenomic dat...   \n",
       "4  infectious bronchitis (ib) causes significant ...   \n",
       "\n",
       "                                           body_text  \\\n",
       "0  vp3, and vp0 (which is further processed to vp...   \n",
       "1  in december 2019, a novel coronavirus, sars-co...   \n",
       "2  the 2019-ncov epidemic has spread across china...   \n",
       "3  metagenomic sequencing, which allows us to dir...   \n",
       "4  infectious bronchitis (ib), which is caused by...   \n",
       "\n",
       "                                   authors  \\\n",
       "0  Joseph C. Ward.  Lidia Lasecka-Dykes...   \n",
       "1          Carla Mavian.  Simone Marini...   \n",
       "2             Hanchu Zhou.  Jianan Yang...   \n",
       "3                  Jiayu Shang.  Yanni Sun   \n",
       "4       Salman L. Butt.  Eric C. Erwood...   \n",
       "\n",
       "                                               title  abstract_word_count  \\\n",
       "0  (The RNA pseudoknots in foot-and-mouth disease...                  194   \n",
       "1  (Regaining perspective on SARS-CoV-2 molecular...                  138   \n",
       "2  (Healthcare-resource-adjusted vulnerabilities ...                   34   \n",
       "3  (CHEER: hierarCHical taxonomic classification ...                  139   \n",
       "4  (Real-time, MinION-based, amplicon sequencing ...                  248   \n",
       "\n",
       "   body_word_count  body_unique_words  \n",
       "0             1709                704  \n",
       "1             2487               1032  \n",
       "2              749                378  \n",
       "3             5153               1371  \n",
       "4             3958               1199  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_covid['body_text'] = df_covid['body_text'].apply(lambda x: str(x).lower())\n",
    "df_covid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuated:\n",
      " during the past three months, a new coronavirus (sars-cov-2) epidemic has been growing exponentially, affecting over 100 thousand people worldwide, and causing enormous distress to economies and societies of affected countries. a plethora of analyses based on viral sequences has already been published, in scientific journals as well as through non-peer reviewed channels, to investigate sars-cov-2 genetic heterogeneity and spatiotemporal dissemination. we examined all full genome sequences currently available to assess the presence of sufficient information for reliable phylogenetic and phylogeographic studies. our analysis clearly shows severe limitations in the present data, in light of which any finding should be considered, at the very best, preliminary and hypothesis-generating. hence the need for avoiding stigmatization based on partial information, and for continuing concerted efforts to increase number and quality of the sequences required for robust tracing of the epidemic. \n",
      "\n",
      "\n",
      "\n",
      "Cleaned:\n",
      " during the past three months a new coronavirus sarscov2 epidemic has been growing exponentially affecting over 100 thousand people worldwide and causing enormous distress to economies and societies of affected countries a plethora of analyses based on viral sequences has already been published in scientific journals as well as through nonpeer reviewed channels to investigate sarscov2 genetic heterogeneity and spatiotemporal dissemination we examined all full genome sequences currently available to assess the presence of sufficient information for reliable phylogenetic and phylogeographic studies our analysis clearly shows severe limitations in the present data in light of which any finding should be considered at the very best preliminary and hypothesisgenerating hence the need for avoiding stigmatization based on partial information and for continuing concerted efforts to increase number and quality of the sequences required for robust tracing of the epidemic\n"
     ]
    }
   ],
   "source": [
    "print(\"Punctuated:\\n\",df_covid['abstract'][1],'\\n\\n\\n')\n",
    "translator = str.maketrans(' ',' ',string.punctuation)\n",
    "print(\"Cleaned:\\n\",df_covid['abstract'][1].translate(translator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed all punctuation from all abstracts. time taken:0:00:00.937033\n"
     ]
    }
   ],
   "source": [
    "start_time= datetime.now()\n",
    "df_covid['abstract'] = df_covid['abstract'].apply(lambda x: str(x).translate(translator))\n",
    "df_covid.head()\n",
    "end_time=datetime.now()\n",
    "print(f'removed all punctuation from all abstracts. time taken:{end_time-start_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed all punctuation from all body_texts. time taken:0:00:27.529968\n"
     ]
    }
   ],
   "source": [
    "start_time= datetime.now()\n",
    "df_covid['body_text'] = df_covid['body_text'].apply(lambda x: str(x).translate(translator))\n",
    "df_covid.head()\n",
    "end_time=datetime.now()\n",
    "print(f'removed all punctuation from all body_texts. time taken:{end_time-start_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping unrelated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the positive stranded rna genomes of picornavi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>during the past three months a new coronavirus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>we integrate the human movement and healthcare...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the fast accumulation of viral metagenomic dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>infectious bronchitis ib causes significant ec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract\n",
       "0  the positive stranded rna genomes of picornavi...\n",
       "1  during the past three months a new coronavirus...\n",
       "2  we integrate the human movement and healthcare...\n",
       "3  the fast accumulation of viral metagenomic dat...\n",
       "4  infectious bronchitis ib causes significant ec..."
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts = df_covid.drop([\"paper_id\",\"body_text\", \"abstract_word_count\", \"body_word_count\",\"body_unique_words\", \"authors\", \"title\"], axis=1)\n",
    "abstracts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vp3 and vp0 which is further processed to vp2 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in december 2019 a novel coronavirus sarscov2 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the 2019ncov epidemic has spread across china ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>metagenomic sequencing which allows us to dire...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>infectious bronchitis ib which is caused by in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           body_text\n",
       "0  vp3 and vp0 which is further processed to vp2 ...\n",
       "1  in december 2019 a novel coronavirus sarscov2 ...\n",
       "2  the 2019ncov epidemic has spread across china ...\n",
       "3  metagenomic sequencing which allows us to dire...\n",
       "4  infectious bronchitis ib which is caused by in..."
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bodytexts = df_covid.drop([\"paper_id\",\"abstract\", \"abstract_word_count\", \"body_word_count\",\"body_unique_words\", \"authors\", \"title\"], axis=1)\n",
    "bodytexts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenized and removed stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.7/site-packages/sklearn/feature_extraction/dict_vectorizer.py:6: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Mapping\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /s/chopin/b/grad/sanketm/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /s/chopin/b/grad/sanketm/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('stopwords') ##Stopword corpora\n",
    "#nltk.download('punkt')  ##Tokenizer\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc):\n",
    "\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "\n",
    "    doc = str(doc).strip()\n",
    "    doc = str(doc).lower()\n",
    "    \n",
    "    translator = str.maketrans(' ',' ',string.punctuation)  #where string.punctuation = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "    doc = str(doc).translate(translator)\n",
    "\n",
    "    # tokenize document\n",
    "\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    word_tokens = word_tokenize(doc) \n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words] \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized and removed stopwords from 10298 abstracts. Time Taken:0:00:22.063956\n"
     ]
    }
   ],
   "source": [
    "start_time= datetime.now()\n",
    "abstract_tokens = abstracts['abstract'].apply(lambda x: normalize_document(x))\n",
    "end_time= datetime.now()\n",
    "print(f'Tokenized and removed stopwords from {len(abstracts)} abstracts. Time Taken:{end_time - start_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [positive, stranded, rna, genomes, picornaviru...\n",
       "1    [past, three, months, new, coronavirus, sarsco...\n",
       "2    [integrate, human, movement, healthcare, resou...\n",
       "3    [fast, accumulation, viral, metagenomic, data,...\n",
       "4    [infectious, bronchitis, ib, causes, significa...\n",
       "Name: abstract, dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract_tokens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing and removing stopwords from body texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time= datetime.now()\n",
    "# body_tokens = bodytexts['body_text'].apply(lambda x: normalize_document(x))\n",
    "# end_time= datetime.now()\n",
    "# print(f'Tokenized and removed stopwords from {len(bodytexts)} body texts. Time Taken:{end_time - start_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# body_tokens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forming n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we form bigrams of our text, both abstracts and body text, in order to be able to use it for better modelling than the frequency based tf-idf vectorizing that we explore in the coming sections.\n",
    "\n",
    "Though TF-IDF is a proven algorithm for topic feature extraction, sometimes we can provide more context to texts by providing sequences of n words at a time. This may allow us to find relationships in a denser feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ref](https://stackoverflow.com/questions/21844546/forming-bigrams-of-words-in-list-of-sentences-with-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ngrams(abstracts['abstract'][0].split(\" \"),2):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ngrams(abstract_tokens[0],2):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Cloud of Abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we visualize the more important terms in the text file by creating a word cloud of the abstracts we have got. [ref](https://www.datacamp.com/community/tutorials/wordcloud-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordcloud as wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with one review:\n",
    "\n",
    "text = abstracts['abstract'][0]\n",
    "\n",
    "# Create and generate a word cloud image:\n",
    "wordcloud = wc.WordCloud().generate(text)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of abstracts: {len(abstracts['abstract'])}\")\n",
    "print(f\"Number of nan abstracts: {abstracts['abstract'].value_counts()['nan']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = abstracts.loc[~(abstracts['abstract'] == 'nan')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of abstracts: {len(abstracts['abstract'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "wordcloud2 = wc.WordCloud(width=1920,height=1080,colormap='cool').generate(' '.join(abstracts['abstract'])) #joining all the abstracts into one huge text block and passing it as input to the wordcloud generator\n",
    "end_time=datetime.now()\n",
    "print(f'generated wordclouds from all {len(abstracts)} abstracts. time taken:{end_time-start_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = plt.figure(figsize=(60, 60))\n",
    "plt.imshow(wordcloud2)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forming Word Vectors for Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we form word vectors of our abstracts and body texts in order to be able to modify the resulting vectors to find similar meaning words.\n",
    "We are using the *Term Frequency- Inverse Document Frequency* calculation to form word vectors. It gives us an idea as to which terms are more frequent/common in the text and which are rarer and seemingly more representative terms in the body text/abstract.\n",
    "\n",
    "Doing this also reduces our feature list to something more reasonable, given that if we look over the entire dataset, there will be upto [13 million](#Statistics) unique features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_tokens.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though we have cleaned the abstracts and body texts and also tokenized them, the TDIDFVectorizer present in SciKit Learn does not need tokenized input, rather the whole collection of abstract texts. We provide the earlier cleaned and processed *abstract* and *bodytexts* DataTables for this purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bodytexts[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts.to_csv(\"./abstracts.csv\", index=False)\n",
    "bodytexts.to_csv(\"./bodytexts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Find top N closest words for given input query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate doc\n",
    "localDoc = abstracts['abstract']\n",
    "doc = sc.parallelize(localDoc).map(lambda line: line.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import Word2Vec\n",
    "start_time = datetime.now()\n",
    "# Learn a mapping from words to Vectors.\n",
    "model = Word2Vec().fit(doc)\n",
    "end_time = datetime.now()\n",
    "print(f'Word2Vec model fit on the {len(abstracts)} abstracts. Time Taken: {end_time - start_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"corona\"\n",
    "topN = 10\n",
    "synonyms = model.findSynonyms(word, topN)\n",
    "result = [(s[0], s[1]) for s in synonyms]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF - SciKitLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Non Parallel implementation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def vectorize(text, max_features):\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(max_features=max_features)\n",
    "    X = vectorizer.fit_transform(text)\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10298, 4096) <class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "text = abstracts['abstract'].values\n",
    "X = vectorize(text, 2 ** 12)\n",
    "print(X.shape,type(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3154)\t0.0663037013816915\n",
      "  (0, 1905)\t0.01830551760193681\n",
      "  (0, 3454)\t0.09676221933595329\n",
      "  (0, 2691)\t0.12066359485327187\n",
      "  (0, 3052)\t0.08749307595622605\n",
      "  (0, 2915)\t0.06382250515143108\n",
      "  (0, 1061)\t0.09969480326093505\n",
      "  (0, 3206)\t0.06221244913660665\n",
      "  (0, 1210)\t0.09111298675133345\n",
      "  (0, 2667)\t0.039873444596741914\n",
      "  (0, 847)\t0.07917816337008543\n",
      "  (0, 2074)\t0.02301517672171394\n",
      "  (0, 3751)\t0.025002007737436393\n",
      "  (0, 2627)\t0.04711287465504985\n",
      "  (0, 3731)\t0.04414069819711706\n",
      "  (0, 1588)\t0.02749229192514992\n",
      "  (0, 4020)\t0.028021071007556703\n",
      "  (0, 1956)\t0.050913869843953116\n",
      "  (0, 2548)\t0.04952097627200816\n",
      "  (0, 4035)\t0.029027014019528324\n",
      "  (0, 292)\t0.04487429681492721\n",
      "  (0, 2133)\t0.08597314005501845\n",
      "  (0, 1592)\t0.09099844048332192\n",
      "  (0, 1790)\t0.042171087899497396\n",
      "  (0, 246)\t0.09692174778056876\n",
      "  :\t:\n",
      "  (0, 3590)\t0.07254609218100817\n",
      "  (0, 1531)\t0.07084513659482568\n",
      "  (0, 865)\t0.15732702682977648\n",
      "  (0, 2106)\t0.11316979525088942\n",
      "  (0, 34)\t0.0772566409716851\n",
      "  (0, 3936)\t0.10919081013162345\n",
      "  (0, 327)\t0.029765597037270733\n",
      "  (0, 1703)\t0.03674067362528182\n",
      "  (0, 1543)\t0.12264324789858913\n",
      "  (0, 3992)\t0.06494297750543465\n",
      "  (0, 1132)\t0.037826858740037154\n",
      "  (0, 3156)\t0.06605552958401116\n",
      "  (0, 3903)\t0.11053518432111657\n",
      "  (0, 336)\t0.1262966657491057\n",
      "  (0, 580)\t0.10028007341691698\n",
      "  (0, 1577)\t0.09724524301224355\n",
      "  (0, 3109)\t0.09384407794447551\n",
      "  (0, 2635)\t0.08357647748640812\n",
      "  (0, 2136)\t0.12347123769123915\n",
      "  (0, 3476)\t0.06441055203736602\n",
      "  (0, 2617)\t0.1259780744999723\n",
      "  (0, 1635)\t0.2991525671904245\n",
      "  (0, 3286)\t0.05382468987479048\n",
      "  (0, 2865)\t0.05757034919210347\n",
      "  (0, 3733)\t0.1617359769512632 (1, 4096)\n"
     ]
    }
   ],
   "source": [
    "print(X[0],X[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = datetime.now()\n",
    "# bodytext = bodytexts['body_text'].values\n",
    "# Xbody = vectorize(bodytext, 2 ** 12)\n",
    "# end_time = datetime.now()\n",
    "# print(f'Generated TF-IDF Calculated DTM for all {len(bodytext)} body texts. time taken:{end_time-start_time}')\n",
    "# print(Xbody.shape,type(Xbody))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Plot this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Paralleizable implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Spark Documentation Reference](https://spark.apache.org/docs/2.2.0/mllib-feature-extraction.html)\n",
    "\n",
    "[Spark implementation reference](https://medium.com/@rezandry/find-most-relevance-text-data-using-pyspark-with-tf-idf-a4269a13e59)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Big Deal ? Spark.ml vs Spark.mllib [link](https://stackoverflow.com/questions/38835829/whats-the-difference-between-spark-ml-and-mllib-packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF\n",
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.ml.feature import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.NullPointerException\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:560)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:422)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-141-d26117106acc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SparkTFIDF\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sspark.kryoserializer.buffer.max\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"1000\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark-2.4.4-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[0;32m--> 136\u001b[0;31m                           conf, jsc, profiler_cls)\n\u001b[0m\u001b[1;32m    137\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;31m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.4.4-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36m_do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# Create the Java SparkContext through Py4J\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjsc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0;31m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.4.4-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36m_initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0mInitialize\u001b[0m \u001b[0mSparkContext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mto\u001b[0m \u001b[0mallow\u001b[0m \u001b[0msubclass\u001b[0m \u001b[0mspecific\u001b[0m \u001b[0minitialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \"\"\"\n\u001b[0;32m--> 306\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJavaSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1523\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1525\u001b[0;31m             answer, self._gateway_client, None, self._fqn)\n\u001b[0m\u001b[1;32m   1526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1527\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.4.4-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.NullPointerException\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:560)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:422)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setAppName(\"SparkTFIDF\")\n",
    "conf.set(\"sspark.kryoserializer.buffer.max\", \"1000\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(abstracts))\n",
    "abstracts[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to work with pyspark and spark APIs, it would be better if we transformed our pandas dataframes to datastructures that it is more comfortable with, such as the SQLcontext Dataframe. This will hopefully make desigining the rest of the tf-idf parallelizable pipeline easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "spdf = sql.SQLContext(sc).createDataFrame(abstracts[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[abstract: string]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(abstract='the positive stranded rna genomes of picornaviruses comprise a single large open reading frame flanked by 5′ and 3′ untranslated regions utrs footandmouth disease virus fmdv has an unusually large 5′ utr 13 kb containing five structural domains these include the internal ribosome entry site ires which facilitates initiation of translation and the cisacting replication element cre less well characterised structures are a 5′ terminal 360 nucleotide stemloop a variable length polyctract of approximately 100200 nucleotides and a series of two to four tandemly repeated pseudoknots pks we investigated the structures of the pks by selective 2′ hydroxyl acetylation analysed by primer extension shape analysis and determined their contribution to genome replication by mutation and deletion experiments shape and mutation experiments confirmed the importance of the previously predicted pk structures for their function deletion experiments showed that although pks are not essential for replication they provide genomes with a competitive advantage however although replicons and fulllength genomes lacking all pks were replication competent no infectious virus was rescued from genomes containing less than one pk copy this is consistent with our earlier report describing the presence of putative packaging signals in the pk region'),\n",
       " Row(abstract='during the past three months a new coronavirus sarscov2 epidemic has been growing exponentially affecting over 100 thousand people worldwide and causing enormous distress to economies and societies of affected countries a plethora of analyses based on viral sequences has already been published in scientific journals as well as through nonpeer reviewed channels to investigate sarscov2 genetic heterogeneity and spatiotemporal dissemination we examined all full genome sequences currently available to assess the presence of sufficient information for reliable phylogenetic and phylogeographic studies our analysis clearly shows severe limitations in the present data in light of which any finding should be considered at the very best preliminary and hypothesisgenerating hence the need for avoiding stigmatization based on partial information and for continuing concerted efforts to increase number and quality of the sequences required for robust tracing of the epidemic'),\n",
       " Row(abstract='we integrate the human movement and healthcare resource data to identify cities with high vulnerability towards the 2019ncov epidemic with respect to available health resources the results inform public health responses in multiple ways'),\n",
       " Row(abstract='the fast accumulation of viral metagenomic data has contributed significantly to new rna virus discovery however the short read size complex composition and large data size can all make taxonomic analysis difficult in particular commonly used alignmentbased methods are not ideal choices for detecting new viral species in this work we present a novel hierarchical classification model named cheer which can conduct readlevel taxonomic classification from order to genus for new species by combining kmer embeddingbased encoding hierarchically organized cnns and carefully trained rejection layer cheer is able to assign correct taxonomic labels for reads from new species we tested cheer on both simulated and real sequencing data the results show that cheer can achieve higher accuracy than popular alignmentbased and alignmentfree taxonomic assignment tools the source code scripts and pretrained parameters for cheer are available via github httpsgithubcomkennthshangcheer'),\n",
       " Row(abstract='infectious bronchitis ib causes significant economic losses in the global poultry industry control of infectious bronchitis is hindered by the genetic diversity of the causative agent infectious bronchitis virus ibv which has led to the emergence of several serotypes that lack complete serologic crossprotection while serotyping by definition requires immunologic characterization genotyping is an efficient means to identify ibvs detected in samples sanger sequencing of the s1 subunit of the spike gene is currently used to genotype ibv however the universal s1 pcr was created to work from cultured ibv and it is inefficient at detecting mixed isolates this paper describes a minionbased ampseq method that genetically typed ibv from clinical samples including samples with multiple isolates total rna was extracted from fifteen tracheal scrapings and choanal cleft swab samples randomly reverse transcribed and pcr amplified using modified s1targeted primers amplicons were barcoded to allow for pooling of samples processed per manufacturer’s instructions into a 1d minion sequencing library and sequenced on the minion the ampseq method detected ibv in 13 of 14 ibvpositive samples ampseq accurately detected and genotyped both ibv lineages in three of five samples containing two ibv lineages additionally one sample contained three ibv lineages and ampseq accurately detected two of the three strain identification including detection of different strains from the same lineage was also possible with this ampseq method the results demonstrate the feasibility of using minionbased ampseq for rapid and accurate identification and lineage typing of ibv from oral swab samples'),\n",
       " Row(abstract='importance as with other traumatic events pandemics such as coronavirus19 covid19 may precipitate or exacerbate psychiatric symptoms such as anxiety and depression while potentially interfering with health systems capacity to treat such symptoms objective to quantify the impact of increasing covid19 infection on extent of psychiatric assessment across 5 eastern massachusetts hospitals design in silico cohort using narrative clinical notes generated between 122020 and 3252020 setting emergency department and outpatient settings from 2 academic medical centers and 3 community hospitals participants all individuals age 13 and older presenting to emergency department or outpatient clinics main outcome or measure documentation of psychiatric symptoms reflecting depression anxiety psychosis or suicide and documentation of violence was drawn from previouslyvalidated term lists results a total of 2483159 outpatient and 205957 emergency department visit notes were analyzed instances of notes referencing depression or anxiety decreased 7581 in outpatient settings with onset of coronavirus in march 2019 and by 4445 in emergency departments in adjusted logistic regression presence of individual psychiatric symptoms in outpatient notes was associated with significant decreases in likelihood of coronavirus testing for depression or0636 95 ci 06060667 conversely presence of violence in an emergency department note was associated with greater likelihood of testing or1487 95 ci 12491761 conclusions and relevance documentation of psychiatric symptoms in both outpatient and emergency department settings diminished sharply in the context of increasing coronavirus infection in massachusetts suggesting that efforts to provide additional resources to manage psychiatric symptoms will be needed funding none'),\n",
       " Row(abstract='nipah virus niv came into limelight recently due to an outbreak in kerala india niv causes severe disease and death in people with over 75 case fatality rate it is a public health concern and has the potential to become a global pandemic lack of treatment has forced the containment methods to be restricted to isolation and surveillance who’s ‘rd blueprint list of priority diseases’ 2018 indicates that there is an urgent need for accelerated research  development for addressing nivmaterials  methodsin the quest for druglike niv inhibitors nvis a thorough literature search followed by systematic data curation was conducted rigorous data analysis was done with curated nvis for prioritizing druglike compounds for the same more than 1800 descriptors of nvis were computed and comparative analysis was performed with the fda approved small molecules and antivirals these compounds were further evaluated through pains filter to study their toxicity profile simultaneously compounds were also prioritized based on the robustness of the assays through which they were identifiedresultsour efforts lead to the creation of a wellcurated structured knowledgebase of 182 nvis with 98 small molecule inhibitors the reported ic50ec50 values for some of these inhibitors are in the nanomolar range – as low as 047 nm in order to prioritize these inhibitors we performed several tests and applied filters to identify druglike nontoxic compounds of 98 a few compounds passed drulito  pains filters exhibiting druglike properties and were also prioritized in an independent screen based only the assay robustness the nvis have diverse structural features and offer a wide spectrum of ways in which they can be developed further as druglike moleculesconclusionwe report a knowledgebase for furthering the development of nvis the platform has a diverse set of 98 nvis of which a few have been prioritized based on a combined evidence strategy the platform has the provision to submit new inhibitors as and when reported by the community for further enhancement of niv inhibitor landscape'),\n",
       " Row(abstract=' a novel coronavirus 2019ncov emerged in wuhan city china at the end of 2019 and has caused an outbreak of humantohuman transmission with a public health emergency of international concern declared by the world health organization on january 30 2020 aim we aimed to estimate the potential risk and geographic range of wuhan novel coronavirus 2019ncov spread within and beyond china from january through to april 2020 methods a series of domestic and international travel networkbased connectivity and risk analyses were performed by using deidentified and aggregated mobile phone data air passenger itinerary data and case reports results the cordon sanitaire of wuhan is likely to have occurred during the latter stages of peak population numbers leaving the city before lunar new year lny with travellers departing into neighbouring cities and other megacities in china we estimated that 59912 air passengers of which 834 95 ui 478  1349 had 2019ncov infection travelled from wuhan to 382 cities outside of mainland china during the two weeks prior to the lockdown of wuhan the majority of these cities were in asia but major hubs in europe the us and australia were also prominent with strong correlation seen between predicted importation risks and reported cases because significant spread has already occurred a large number of airline travellers 33 million under the scenario of 75 travel reduction from normal volumes may be required to be screened at origin highrisk cities in china and destinations across the globe for the following three months of february to april 2020 to effectively limit spread beyond its current extent conclusion further spread of 2019ncov within china and international exportation is likely to occur all countries especially vulnerable regions should be prepared for efforts to contain the 2019ncov infection'),\n",
       " Row(abstract='introduction recent events highlight how emerging and reemerging pathogens are becoming global challenges for public health in december 2019 a novel coronavirus has emerged this has suddenly turned out into global health concern objectives aim of this research is to focus on the bibliometric aspects in order to measure what is published in the first 30days of a global epidemic outbreak methods we searched pubmed database in order to find all relevant studies in the first 30days from the first publication results from the initial 442 identified articles 234 were read inextenso the majority of papers come from china uk and usa 637 of the papers were commentaries editorials and reported data and only 175 of the sources used data directly collected on the field topics mainly addressed were epidemiology preparedness and generic discussion nnr showed a reduction for both the objectives assessed from january to february conclusions diagnosis and effective preventive and therapeutic measures were the fields in which more research is still needed the vast majority of scientific literature in the first 30days of an epidemic outbreak is based on reported data rather than primary data nevertheless the scientific statements and public health decisions rely on these data'),\n",
       " Row(abstract='faced with the current largescale public health emergency collecting sorting and analyzing biomedical information related to the coronavirus should be done as quickly as possible to gain a global perspective which is a basic requirement for strengthening epidemic control capacity however for human researchers studying the viruses and the hosts the vast amount of information available cannot be processed effectively and in a timely manner particularly when the scientific understanding may be limited which can further lower the information processing efficiency we present twirls a method that can automatically acquire organize and classify information additionally independent functional data sources can be added to build an inference system using a machinebased approach which can provide relevant knowledge to help human researchers quickly establish subject cognition and to make more effective decisions twirls can automatically analyze more than three million words in more than 14000 literature articles in only 4 hours combining with generalized gene interaction databases creates a data interface that can help researchers to further analyze the information using the twirls system we found that an important regulatory factor angiotensinconverting enzyme 2 ace2 may be involved in the host pathological changes on binding to the coronavirus after infection after triggering functional changes in ace2at2r an imbalance in the steadystate cytokine regulatory axis involving the reninangiotensin system and ip10 leads to a cytokine storm'),\n",
       " Row(abstract='viruses interact with hundreds to thousands of proteins in mammals yet adaptation against viruses has only been studied in a few proteins specialized in antiviral defense whether adaptation to viruses typically involves only specialized antiviral proteins or affects a broad array of proteins is unknown here we analyze adaptation in 1300 virusinteracting proteins manually curated from a set of 9900 proteins conserved across mammals we show that viruses i use the more evolutionarily constrained proteins from the cellular functions they hijack and that ii despite this high constraint virusinteracting proteins account for a high proportion of all protein adaptation in humans and other mammals adaptation is elevated in virusinteracting proteins across all functional categories including both immune and nonimmune functions our results demonstrate that viruses are one of the most dominant drivers of evolutionary change across mammalian and human proteomes'),\n",
       " Row(abstract='school closure is often considered as an option to mitigate influenza epidemics because of its potential to reduce transmission in children and then in the community the policy is still however highly debated because of controversial evidence moreover the specific mechanisms leading to mitigation are not clearly identifiedwe introduced a stochastic spatial agespecific metapopulation model to assess the role of holidayassociated behavioral changes and how they affect seasonal influenza dynamics the model is applied to belgium parameterized with countryspecific data on social mixing and travel and calibrated to the 20082009 influenza season it includes behavioral changes occurring during weekend vs weekday and holiday vs schoolterm several experimental scenarios are explored to identify the relevant social and behavioral mechanismsstochastic numerical simulations show that holidays considerably delay the peak of the season and mitigate its impact changes in mixing patterns are responsible for the observed effects whereas changes in travel behavior do not alter the epidemic weekends are important in slowing down the season by periodically dampening transmission christmas holidays have the largest impact on the epidemic however later school breaks may help in reducing the epidemic size stressing the importance of considering the full calendar an extension of the christmas holiday of 1 week may further mitigate the epidemicchanges in the way individuals establish contacts during holidays are the key ingredient explaining the mitigating effect of regular school closure our findings highlight the need to quantify these changes in different demographic and epidemic contexts in order to provide accurate and reliable evaluations of closure effectiveness they also suggest strategic policies in the distribution of holiday periods to minimize the epidemic impact'),\n",
       " Row(abstract='posttranscriptional gene silencing ptgs is a powerful tool to understand and control plant metabolic pathways which is central to plant biotechnology ptgs is commonly accomplished through delivery of small interfering rna sirna into cells while sirna delivery has been optimized for mammalian systems it remains a significant challenge for plants due to the plant cell wall standard plant sirna delivery methods agrobacterium and viruses involve coding sirna into dna vectors and are only tractable for certain plant species herein we develop a nanotubebased platform for direct delivery of sirna and show high silencing efficiency in intact plant cells we demonstrate that nanotubes successfully deliver sirna and silence endogenous genes owing to effective intracellular delivery and nanotubeinduced protection of sirna from nuclease degradation this study establishes that nanotubes which are below the size exclusion limit of the plant cell wall could enable a myriad of plant biotechnology applications that rely on rna delivery'),\n",
       " Row(abstract='during outbreaks of emerging infections the lack of effective drugs and vaccines increases reliance on nonpharmacologic public health interventions and behavior change to limit humantohuman transmission interventions that increase the speed with which infected individuals remove themselves from the susceptible population are paramount particularly isolation and hospitalization ebola virus disease evd severe acute respiratory syndrome sars and middle east respiratory syndrome mers are zoonotic viruses that have caused significant recent outbreaks with sustained humantohuman transmission this investigation quantified changing mean removal rates mrr and days from symptom onset to hospitalization dsoh of infected individuals from the population in seven different outbreaks of evd sars and mers to test for statistically significant differences in these metrics between outbreaks we found that epidemic week and viral serial interval were correlated with the speed with which populations developed and maintained health behaviors in each outbreak'),\n",
       " Row(abstract='recent outbreak of coronavirus disease 2019 covid19 in china has lead a global pandemic around the world for controlling covid19 outbreaks most countries take two typical intervention strategies suppression approach like immediately lockdowning cities at epicentre of and mitigation or mitigation approach that slows down but not stopping epidemic of covid19 for reducing peak healthcare demand both strategies have their apparent merits and limitations it becomes extremely hard to conduct one intervention strategy as the most feasible way to certain country targeting at this problem this paper conducts a feasibility study by defining a mathematical model named semcr that access the effectiveness of mitigation suppression and hybrid interventions for controlling covid19 outbreaks in london and wuhan the model first extends traditional seir susceptibleexposedinfectiousrecovered model by considering one key fact there is a direct link between exposed and recovered population then it defines threshold and parameters to classify two stages of covid control active contain by isolation of cases and contacts passive contain by suppression or mitigation the model was fitted and evaluated with public dataset containing daily number of confirmed active cases including wuhan london hubei province and the uk during january 2020 and march 2020 the simulated results show that 1 suppression taken in wuhan significantly reduces total exposed and infectious populations but it has to be consistently maintained at least a period of 90 days by the middle of april 2020 where its success relies on sufficient support from other parts of china 2 in london it is possible to take a hybrid intervention of suppression and mitigation for every 2 or 3 weeks over a longer period while the total infectious populations will be doubled the economic performance of london will be less affected due to limited mobility constraints 3 both in wuhan and london cases one important issue of fitting practical data is that there are a large portion like 425 in wuhan of selfrecovered population who are asymptomatic or mild symptomatic these people might think they have been healthy at home because they did not go to hospital for covid19 tests early release of intervention intensity might increase a risk of the second breakout one limitation of our model is that its prediction of infections and deaths depends on a parameter estimation of intervention intensity that presented by averagenumber contacts with susceptible individuals as infectious individuals in a certain region we assume that each intervention has the same effect on the reproduction number in different regions over time the practical effectiveness of implementing intervention intensity might be varied with respect to cultures or other issues of certain county'),\n",
       " Row(abstract='quantitative reverse transcription polymerase chain reaction rtqpcr assay is the gold standard recommended to test for acute sarscov2 infection it has been used by the centers for disease control and prevention cdc and several other companies in their emergency use authorization eua assays with many pcrbased molecular assays an extraction step is routinely used as part of the protocol this step can take up a significant amount of time and labor especially if the extraction is performed manually long assay time partly caused by slow sample preparation steps has created a large backlog when testing patient samples suspected of covid19 using flu and rsv clinical specimens we have collected evidence that the rtqpcr assay can be performed directly on patient sample material from a nasal swab immersed in virus transport medium vtm without an rna extraction step we have also used this approach to test for the direct detection of sarscov2 reference materials spiked in vtm our data while preliminary suggest that using a few microliters of these untreated samples still can lead to sensitive test results if rna extraction steps can be omitted without significantly affecting clinical sensitivity the turnaround time of covid19 tests and the backlog we currently experience can be reduced drastically next we will confirm our findings using patient samples'),\n",
       " Row(abstract='the spread of covid19 engulfs almost all the countries and territories of the planet and infections and fatality are increasing rapidly the first epicenter of its massive spread was in wuhan hubei province china having a temperate weather but the spread has got an unprecedented momentum in european temperate countries mainly in italy and spain as of march 30 2020 however malaysia and singapore and the neighboring tropical countries of china got relatively low spread and fatality that created a research interest on whether there are potential impacts of weather condition on covid19 spread adopting the sir susceptible infected removed deviated model to predict potential cases and death in the coming days from covid19 was done using the secondary and official sources of data this study shows that covid19 spread and fatality tend to be high across the world but compared to tropical countries it is going to be incredibly high in the temperate countries having lower temperature 716°c and humidity 8090 in last march however some literature predicted that this might not to be true rather irrespective of weather conditions there might be a continuous spread and death moreover a large number of asymptotic covid19 carrier in both temperate and tropical countries may reoutbreak in the coming winter therefore a comprehensive global program with the leadership of who for testing of entire population of the world is required which will be very useful for the individual states to take proper political action social movement and medical services'),\n",
       " Row(abstract='human astroviruses are small nonenveloped viruses with positivesense singlestranded rna genomes that contain three main open reading frames orf1a orf1b and orf2 astroviruses cause acute gastroenteritis in children worldwide and have been associated with encephalitis and meningitis in immunocompromised individuals through comparative genomic analysis of 400 astrovirus sequences we identified a conserved “orfx” overlapping the capsidencoding orf2 in genogroup i iii and iv astroviruses orfx appears to be subject to purifying selection consistent with it encoding a functional protein product termed xp using ribosome profiling of cells infected with human astrovirus 1 we confirm initiation at the orfx aug xpknockout astroviruses are strongly attenuated and after passaging can partly restore viral titer via pseudoreversions thus demonstrating that xp plays an important role in virus growth to further investigate xp we developed an astrovirus replicon system we demonstrate that xp has only minor effects on rna replication and structural protein production instead xp associates with the plasma membrane with an extracellular nterminus topology and promotes efficient virus release using two different assays we show that expression of human or related astrovirus xps leads to cell permeabilization suggesting a viroporinlike activity the discovery of xp advances our knowledge of these important human viruses and opens a new direction of research into astrovirus replication and pathogenesis'),\n",
       " Row(abstract='ribosomal frameshifting during the translation of rna is implicated in both human disease and viral infection while previous work has uncovered many mechanistic details about single rna frameshifting kinetics in vitro very little is known about how single rna frameshift in living systems to confront this problem we have developed technology to quantify livecell single rna translation dynamics in frameshifted open reading frames applying this technology to rna encoding the hiv1 frameshift sequence reveals a small subset 8 of the translating pool robustly frameshift in living cells frameshifting rna are preferentially in multirna “translation factories” are translated at about the same rate as nonframeshifting rna 2 aasec and can continuously frameshift for more than four rounds of translation fits to a bursty model of frameshifting constrain frameshifting kinetic rates and demonstrate how ribosomal traffic jams contribute to the persistence of the frameshifting state these data provide novel insight into retroviral frameshifting and could lead to new strategies to perturb the process in living cells'),\n",
       " Row(abstract='since the introduction of the novel corona virus the covid 19 to the chinese city wuhan in the hubei province during the late december 2019 the effectiveness of the deadly disease its human infection spreading severity and the mortality rate of the infection has been an issue of debate the outbreak of the virus along the time has become a massive threat to the global public health security and has been declared as a pandemic accounting the radical number of increases in the infected cases and the death due to covid 19 infections around the globe there is a need to predict the infections among the people by making proper optimization and using various infectious disease modelling idm methods in order to challenge the outcome in comparison with previous diseases like sars and ebola viruses the new corona virus covid 19 infections are infectious during the incubation period in addition to that naturally produced droplets from humans eg droplets produced by breathing talking sneezing coughing and persontoperson contact transmission are reported to be the foremost ways of transmission of novel corona virus by considering the above two factors a modified seir susceptibility exposure infection recovery method have been used for predicting the spread of the infections in the state of tamil nadu which is located in the southern part of india further we have utilized the current surveillance data from health and family welfare department government of tamil nadu to accurately predict the spreading trend of the infection on a state level')]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spdf.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "again here, the HashingTF(term frequency hashing class) requires a string input, so we will again use the *abstracts* and the *bodytexts* datatables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline reference:[link](https://spark.apache.org/docs/latest/ml-pipeline.html#example-pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"abstract\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"rawFeatures\")\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\",minDocFreq=2)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.ml.pipeline.PipelineModel'> <class 'pyspark.sql.dataframe.DataFrame'> \n",
      "TFIDF computed. Time taken: 0:00:05.040755\n"
     ]
    }
   ],
   "source": [
    "starttime = datetime.now()\n",
    "pipelinemodel = pipeline.fit(spdf)\n",
    "training_transform = pipelinemodel.transform(spdf)\n",
    "endtime = datetime.now()\n",
    "print(type(model),type(training_transform),f'\\nTFIDF computed. Time taken: {endtime-starttime}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[abstract: string, words: array<string>, rawFeatures: vector, features: vector]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA  & Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import HashingTF, IDF\n",
    "\n",
    "# Load documents (one per line).\n",
    "documents = sc.parallelize(abstracts['abstract'][:10]).map(lambda line: line.split(\" \"))\n",
    "\n",
    "hashingTF = HashingTF(numFeatures = 2 ** 12)\n",
    "tf = hashingTF.transform(documents)\n",
    "\n",
    "# While applying HashingTF only needs a single pass to the data, applying IDF needs two passes:\n",
    "# First to compute the IDF vector and second to scale the term frequencies by IDF.\n",
    "tf.cache()\n",
    "idf = IDF().fit(tf)\n",
    "tfidf = idf.transform(tf)\n",
    "\n",
    "# spark.mllib's IDF implementation provides an option for ignoring terms\n",
    "# which occur in less than a minimum number of documents.\n",
    "# In such cases, the IDF for these terms is set to 0.\n",
    "# This feature can be used by passing the minDocFreq value to the IDF constructor.\n",
    "idfIgnore = IDF(minDocFreq=2).fit(tf)\n",
    "tfidfIgnore = idfIgnore.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected the RDDs to caclulate shape of TF-IDF matrix. Shape(10, 4096):. time taken:0:00:02.157212\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "import numpy as np\n",
    "tempnparray = np.array(tfidfIgnore.collect())\n",
    "end_time = datetime.now()\n",
    "print(f'Collected the RDDs to caclulate shape of TF-IDF matrix. Shape{tempnparray.shape}:. time taken:{end_time - start_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Reqwriting using pysparks ml library, \n",
    "# from pyspark.ml.feature import PCA as PCAml\n",
    "# from pyspark.ml.linalg import Vectors\n",
    "# pca = PCAml(k=2, inputCol=\"features\", outputCol=\"pca\")\n",
    "# model = pca.fit(training_transform)\n",
    "# pcatransformed = model.transform(training_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1962.computePrincipalComponents.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 15.0 failed 4 times, most recent failure: Lost task 0.3 in stage 15.0 (TID 139, 129.82.44.162, executor 3): org.apache.spark.SparkException: Kryo serialization failed: Buffer overflow. Available: 7, required: 8\nSerialization trace:\ndata$mcD$sp (breeze.linalg.DenseVector$mcD$sp). To avoid this, increase spark.kryoserializer.buffer.max value.\n\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:354)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:456)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: com.esotericsoftware.kryo.KryoException: Buffer overflow. Available: 7, required: 8\nSerialization trace:\ndata$mcD$sp (breeze.linalg.DenseVector$mcD$sp)\n\tat com.esotericsoftware.kryo.io.Output.require(Output.java:167)\n\tat com.esotericsoftware.kryo.io.Output.writeLong(Output.java:530)\n\tat com.esotericsoftware.kryo.io.Output.writeDouble(Output.java:659)\n\tat com.esotericsoftware.kryo.io.Output.writeDoubles(Output.java:740)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$DoubleArraySerializer.write(DefaultArraySerializers.java:211)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$DoubleArraySerializer.write(DefaultArraySerializers.java:200)\n\tat com.esotericsoftware.kryo.Kryo.writeObjectOrNull(Kryo.java:629)\n\tat com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:86)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:508)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:351)\n\t... 4 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n\tat org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1092)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1161)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1137)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computeGramianMatrix(RowMatrix.scala:122)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computeCovariance(RowMatrix.scala:358)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computePrincipalComponentsAndExplainedVariance(RowMatrix.scala:401)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computePrincipalComponents(RowMatrix.scala:425)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.SparkException: Kryo serialization failed: Buffer overflow. Available: 7, required: 8\nSerialization trace:\ndata$mcD$sp (breeze.linalg.DenseVector$mcD$sp). To avoid this, increase spark.kryoserializer.buffer.max value.\n\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:354)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:456)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\nCaused by: com.esotericsoftware.kryo.KryoException: Buffer overflow. Available: 7, required: 8\nSerialization trace:\ndata$mcD$sp (breeze.linalg.DenseVector$mcD$sp)\n\tat com.esotericsoftware.kryo.io.Output.require(Output.java:167)\n\tat com.esotericsoftware.kryo.io.Output.writeLong(Output.java:530)\n\tat com.esotericsoftware.kryo.io.Output.writeDouble(Output.java:659)\n\tat com.esotericsoftware.kryo.io.Output.writeDoubles(Output.java:740)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$DoubleArraySerializer.write(DefaultArraySerializers.java:211)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$DoubleArraySerializer.write(DefaultArraySerializers.java:200)\n\tat com.esotericsoftware.kryo.Kryo.writeObjectOrNull(Kryo.java:629)\n\tat com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:86)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:508)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:351)\n\t... 4 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-135-61224ee62ae6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Compute the top (9 principal components.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Principal components are stored in a local dense matrix.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mpc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomputePrincipalComponents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Project the rows to the linear space spanned by the top (9 principal components.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.4.4-bin-hadoop2.7/python/pyspark/mllib/linalg/distributed.py\u001b[0m in \u001b[0;36mcomputePrincipalComponents\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m    368\u001b[0m         DenseVector([-4.6102, -4.9745])]\n\u001b[1;32m    369\u001b[0m         \"\"\"\n\u001b[0;32m--> 370\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_matrix_wrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"computePrincipalComponents\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'2.2.0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.4.4-bin-hadoop2.7/python/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, name, *a)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;34m\"\"\"Call method of java_model\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.4.4-bin-hadoop2.7/python/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[0;34m(sc, func, *args)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.4.4-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1962.computePrincipalComponents.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 15.0 failed 4 times, most recent failure: Lost task 0.3 in stage 15.0 (TID 139, 129.82.44.162, executor 3): org.apache.spark.SparkException: Kryo serialization failed: Buffer overflow. Available: 7, required: 8\nSerialization trace:\ndata$mcD$sp (breeze.linalg.DenseVector$mcD$sp). To avoid this, increase spark.kryoserializer.buffer.max value.\n\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:354)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:456)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: com.esotericsoftware.kryo.KryoException: Buffer overflow. Available: 7, required: 8\nSerialization trace:\ndata$mcD$sp (breeze.linalg.DenseVector$mcD$sp)\n\tat com.esotericsoftware.kryo.io.Output.require(Output.java:167)\n\tat com.esotericsoftware.kryo.io.Output.writeLong(Output.java:530)\n\tat com.esotericsoftware.kryo.io.Output.writeDouble(Output.java:659)\n\tat com.esotericsoftware.kryo.io.Output.writeDoubles(Output.java:740)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$DoubleArraySerializer.write(DefaultArraySerializers.java:211)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$DoubleArraySerializer.write(DefaultArraySerializers.java:200)\n\tat com.esotericsoftware.kryo.Kryo.writeObjectOrNull(Kryo.java:629)\n\tat com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:86)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:508)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:351)\n\t... 4 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n\tat org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1092)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1161)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1137)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computeGramianMatrix(RowMatrix.scala:122)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computeCovariance(RowMatrix.scala:358)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computePrincipalComponentsAndExplainedVariance(RowMatrix.scala:401)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computePrincipalComponents(RowMatrix.scala:425)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.SparkException: Kryo serialization failed: Buffer overflow. Available: 7, required: 8\nSerialization trace:\ndata$mcD$sp (breeze.linalg.DenseVector$mcD$sp). To avoid this, increase spark.kryoserializer.buffer.max value.\n\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:354)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:456)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\nCaused by: com.esotericsoftware.kryo.KryoException: Buffer overflow. Available: 7, required: 8\nSerialization trace:\ndata$mcD$sp (breeze.linalg.DenseVector$mcD$sp)\n\tat com.esotericsoftware.kryo.io.Output.require(Output.java:167)\n\tat com.esotericsoftware.kryo.io.Output.writeLong(Output.java:530)\n\tat com.esotericsoftware.kryo.io.Output.writeDouble(Output.java:659)\n\tat com.esotericsoftware.kryo.io.Output.writeDoubles(Output.java:740)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$DoubleArraySerializer.write(DefaultArraySerializers.java:211)\n\tat com.esotericsoftware.kryo.serializers.DefaultArraySerializers$DoubleArraySerializer.write(DefaultArraySerializers.java:200)\n\tat com.esotericsoftware.kryo.Kryo.writeObjectOrNull(Kryo.java:629)\n\tat com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:86)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:508)\n\tat com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)\n\tat org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:351)\n\t... 4 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "\n",
    "rows = sc.parallelize(tfidfIgnore.collect())\n",
    "\n",
    "mat = RowMatrix(rows)\n",
    "\n",
    "# Compute the top (9 principal components.\n",
    "# Principal components are stored in a local dense matrix.\n",
    "pc = mat.computePrincipalComponents(9)\n",
    "\n",
    "# Project the rows to the linear space spanned by the top (9 principal components.\n",
    "projected = mat.multiply(pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected.numRows(), projected.numCols()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To separate the literature, k-means will be run on the vectorized text. Given the number of clusters, k, k-means will categorize each vector by taking the mean distance to a randomly initialized centroid. The centroids are updated iteratively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elbow method to determine the optimal number of clusters for k-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import KMeans\n",
    "\n",
    "cost = np.zeros(20)\n",
    "for k in range(2, 20):\n",
    "    model = KMeans.train(projected.rows,\n",
    "                         k,\n",
    "                         maxIterations=10,\n",
    "                         initializationMode=\"random\",\n",
    "                         seed=50,\n",
    "                         initializationSteps=5,\n",
    "                         epsilon=1e-4)\n",
    "\n",
    "    cost[k] = model.computeCost(projected.rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sbs\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize =(8,6))\n",
    "ax.plot(range(2,20), cost[2:20])\n",
    "ax.set_xlabel('k')\n",
    "ax.set_ylabel('cost')\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate clustering by computing Within Set Sum of Squared Errors\n",
    "def error(point):\n",
    "    center = clusters.centers[clusters.predict(point)]\n",
    "    return sqrt(sum([x**2 for x in (point - center)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "\n",
    "cluster_centers = []\n",
    "\n",
    "# Build the model (cluster the data)\n",
    "for k in range(2, 20):\n",
    "    clusters = KMeans.train(projected.rows,\n",
    "                            k,\n",
    "                            maxIterations=10,\n",
    "                            initializationMode=\"random\",\n",
    "                            seed=50,\n",
    "                            initializationSteps=5,\n",
    "                            epsilon=1e-4)\n",
    "    \n",
    "    # Evaluate clustering by computing Within Set Sum of Squared Errors\n",
    "    def error(point):\n",
    "        center = clusters.centers[clusters.predict(point)]\n",
    "        return sqrt(sum([x**2 for x in (point - center)]))\n",
    "\n",
    "    WSSSE = projected.rows.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "    print(\"The cluster {} has Within Set Sum of Squared Error {}\".format(k, WSSSE))\n",
    "    \n",
    "    centers = clusters.clusterCenters\n",
    "    \n",
    "    cluster_centers.append(centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cluster Centers: \")\n",
    "for center in cluster_centers:\n",
    "    #print(center)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected.rows.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_split_int = projected.rows.map(lambda x: [float(x[0]), float(x[1]), float(x[2]), float(x[3]), float(x[4]), float(x[5]), float(x[6]), float(x[7]), float(x[8])])\n",
    "\n",
    "# Convert rdd_split_int RDD into Spark DataFrame\n",
    "rdd_split_int_df = spark.createDataFrame(rdd_split_int, schema=[\"col1\", \"col2\", \"col3\", \"col4\", \"col5\", \"col6\", \"col7\", \"col8\", \"col9\"])\n",
    "\n",
    "# Convert Spark DataFrame into Pandas DataFrame\n",
    "rdd_split_int_df_pandas = rdd_split_int_df.toPandas()\n",
    "\n",
    "# Convert \"cluster_centers\" that you generated earlier into Pandas DataFrame\n",
    "cluster_centers_pandas = pd.DataFrame(cluster_centers[8], columns=[\"col1\", \"col2\", \"col3\", \"col4\", \"col5\", \"col6\", \"col7\", \"col8\", \"col9\"])\n",
    "\n",
    "# Create an overlaid scatter plot\n",
    "plt.scatter(rdd_split_int_df_pandas, cluster_centers_pandas, color=\"red\", marker=\"x\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ref - Scikit learn](https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html#sphx-glr-auto-examples-text-plot-document-clustering-py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow method and Silhouette method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification and cross validation using Multi-Layer Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F Score, Precision, Recall, and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to print out classification model report\n",
    "def classification_report(model_name, test, pred):\n",
    "    from sklearn.metrics import precision_score, recall_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import f1_score\n",
    "    \n",
    "    print(model_name, \":\\n\")\n",
    "    print(\"Accuracy Score: \", '{:,.3f}'.format(float(accuracy_score(test, pred)) * 100), \"%\")\n",
    "    print(\"     Precision: \", '{:,.3f}'.format(float(precision_score(test, pred, average='macro')) * 100), \"%\")\n",
    "    print(\"        Recall: \", '{:,.3f}'.format(float(recall_score(test, pred, average='macro')) * 100), \"%\")\n",
    "    print(\"      F1 score: \", '{:,.3f}'.format(float(f1_score(test, pred, average='macro')) * 100), \"%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# test set size of 20% of the data and the random seed 42 <3\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.toarray(),y_pred, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"X_train size:\", len(X_train))\n",
    "print(\"X_test size:\", len(X_test), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(400, 100), max_iter=200, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "clf.fit(X_train, y_train)\n",
    "end_time = datetime.now()\n",
    "print(f'training time taken:{end_time-start_time}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "pred = cross_val_predict(clf, X_train, y_train, n_jobs=-1)\n",
    "end_time = datetime.now()\n",
    "print(f'cross validation time taken:{end_time-start_time}')\n",
    "classification_report(\"MLP Report (Training Set)\", y_train, sgd_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "pred = cross_val_predict(clf, X_test, y_test, cv=3, n_jobs=-1)\n",
    "end_time = datetime.now()\n",
    "print(f'test time taken:{end_time-start_time}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report(\"MLP Report (Test Set)\", y_test, sgd_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_score = cross_val_score(clf, X.toarray(), y_pred, cv=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean cv Score - MLP: {:,.3f}\".format(float(cv_score.mean()) * 100), \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization and stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://cdn.oreillystatic.com/en/assets/1/event/261/Text%20analytics%20and%20new%20visualization%20techniques%20Presentation.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [COVID-19 Open Research Dataset Challenge (CORD-19)](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/)\n",
    "2. [How to build a topic-based search engine](https://www.smithinst.co.uk/insights/build-topic-based-search-engine/)\n",
    "3. W. Buntine et al., \"A Scalable Topic-Based Open Source Search Engine,\" IEEE/WIC/ACM International Conference on Web Intelligence (WI'04), Beijing, China, 2004, pp. 228-234.\n",
    "4. [Semantic Topic Modeling for Search Queries at Google](https://gofishdigital.com/semantic-topic-modeling/)\n",
    "5. GRANT, C.; PAZHAYIDAM GEORGE, C.; KANJILAL, V.; NIRKHIWALE, S.; WILSON, J.; WANG, D.. A Topic-Based Search, Visualization, and Exploration System. Florida Artificial Intelligence Research Society Conference, North America, apr. 2015. Available at: <https://www.aaai.org/ocs/index.php/FLAIRS/FLAIRS15/paper/view/10445/10360>. Date accessed: 25 Apr. 2020."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288.72px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
